{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292adf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# Path to image folders, \n",
    "#data_path = fr'../Datasets/config'\n",
    "data_path = fr'../Datasets/full/Training'\n",
    "\n",
    "def remove_white_background(pixels):\n",
    "    newPixels = []\n",
    "    for pixel in pixels:\n",
    "        pixel = list(pixel)\n",
    "        if ((256 > pixel[0] > 200) and (256 > pixel[1] > 200) and (256 > pixel[2] > 200)):\n",
    "            pixel[0] = 0\n",
    "            pixel[1] = 0\n",
    "            pixel[2] = 0\n",
    "        newPixels.append(pixel)\n",
    "    \n",
    "    return newPixels\n",
    "\n",
    "\n",
    "def redify(pixels):\n",
    "    return [r for r, g, b in pixels]\n",
    "\n",
    "                \n",
    "def greenify(pixels):\n",
    "    return [g for r, g, b in pixels]\n",
    "\n",
    "\n",
    "def blueify(pixels):\n",
    "    return [b for r, g, b in pixels]\n",
    "\n",
    "\n",
    "def get_rgb_pixels_onehot_labels(src):\n",
    "    print(\"Starting...\")\n",
    "    newPixels = []\n",
    "    y = np.empty(shape=[0, 1])\n",
    "\n",
    "    for subdir in os.listdir(src):\n",
    "        current_path = os.path.join(src, subdir)\n",
    "        for file in os.listdir(current_path):\n",
    "            img = Image.open(os.path.join(current_path, file))\n",
    "            imgResize = img.resize((24,24))\n",
    "            pixels = list(imgResize.getdata())\n",
    "            pixels = remove_white_background(pixels)\n",
    "            newPixels.append(pixels)\n",
    "            y = np.append(y, subdir)\n",
    "    return newPixels, LabelBinarizer().fit_transform(y) # OneHot encode y\n",
    "\n",
    "\n",
    "def process_files(src):\n",
    "    X_red_train = []\n",
    "    X_red_validation = []\n",
    "    X_red_test = []\n",
    "    X_green_train = []\n",
    "    X_green_validation = []\n",
    "    X_green_test = []\n",
    "    X_blue_train = []\n",
    "    X_blue_validation = []\n",
    "    X_blue_test = []\n",
    "    all_pixels, y = get_rgb_pixels_onehot_labels(src)\n",
    "\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(all_pixels, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "    \n",
    "    for pixels in X_train:       \n",
    "        X_red_train.append(redify(pixels.copy()))\n",
    "        X_green_train.append(greenify(pixels.copy()))\n",
    "        X_blue_train.append(blueify(pixels.copy()))\n",
    "        \n",
    "    for pixels in X_validation:       \n",
    "        X_red_validation.append(redify(pixels.copy()))\n",
    "        X_green_validation.append(greenify(pixels.copy()))\n",
    "        X_blue_validation.append(blueify(pixels.copy()))\n",
    "        \n",
    "    for pixels in X_test:       \n",
    "        X_red_test.append(redify(pixels.copy()))\n",
    "        X_green_test.append(greenify(pixels.copy()))\n",
    "        X_blue_test.append(blueify(pixels.copy()))\n",
    "    \n",
    "    \n",
    "    print(\"Finished \\n\")\n",
    "    return np.asarray(X_red_train), np.asarray(X_red_validation), np.asarray(X_red_test), np.asarray(X_green_train), np.asarray(X_green_validation), np.asarray(X_green_test), np.asarray(X_blue_train), np.asarray(X_blue_validation), np.asarray(X_blue_test), y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "def get_youdens_index(predictions, Y):\n",
    "    # Calculate true positive/negative and false positive/negative\n",
    "    tp = sum((Y == predictions) * (Y == 1) * 1)\n",
    "    tn = sum((Y == predictions) * (Y == 0) * 1)\n",
    "    fp = sum((Y != predictions) * (Y == 0) * 1)\n",
    "    fn = sum((Y != predictions) * (Y == 1) * 1)\n",
    "    \n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    result = sensitivity - (1 - specificity)\n",
    "    # Put it in a dateframe for nicer visuals\n",
    "    df = pd.DataFrame({'Youdens Index': result})\n",
    "    pd.set_option('display.max_rows', 200)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a157e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Finished \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call process_files and assign variables\n",
    "X_red_train, X_red_validation, X_red_test, X_green_train, X_green_validation, X_green_test, X_blue_train, X_blue_validation, X_blue_test, Y_train, Y_validation, Y_test = process_files(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4666de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Model Validation\n",
      "     Youdens Index\n",
      "0         0.953750\n",
      "1         0.974566\n",
      "2         0.897540\n",
      "3         0.923103\n",
      "4         0.945624\n",
      "5         0.923103\n",
      "6         0.974569\n",
      "7         0.884599\n",
      "8         0.976763\n",
      "9         0.886228\n",
      "10        0.923159\n",
      "11        0.907836\n",
      "12        0.937359\n",
      "13        0.937809\n",
      "14        0.982400\n",
      "15        0.930849\n",
      "16        0.930232\n",
      "17        0.892226\n",
      "18        0.846543\n",
      "19        0.807156\n",
      "20        0.983459\n",
      "21        0.923103\n",
      "22        0.992366\n",
      "23        0.953862\n",
      "24        0.885160\n",
      "25        0.822742\n",
      "26        0.992254\n",
      "27        0.908010\n",
      "28        0.979527\n",
      "29        0.984621\n",
      "30        0.976875\n",
      "31        0.976987\n",
      "32        0.792491\n",
      "33        0.999888\n",
      "34        0.800068\n",
      "35        0.916611\n",
      "36        0.910345\n",
      "37        0.865826\n",
      "38        0.934654\n",
      "39        0.884879\n",
      "40        0.927607\n",
      "41        0.967183\n",
      "42        0.897951\n",
      "43        0.931017\n",
      "44        0.995859\n",
      "45        0.869387\n",
      "46        1.000000\n",
      "47        0.984733\n",
      "48        0.992254\n",
      "49        0.952269\n",
      "50        0.992310\n",
      "51        0.999944\n",
      "52        0.953301\n",
      "53        0.894320\n",
      "54        0.892849\n",
      "55        0.976538\n",
      "56        0.935047\n",
      "57        0.895984\n",
      "58        0.969129\n",
      "59        0.961552\n",
      "60        0.992366\n",
      "61        0.876628\n",
      "62        0.915357\n",
      "63        0.946284\n",
      "64        0.923159\n",
      "65        0.912113\n",
      "66        0.838499\n",
      "67        0.915470\n",
      "68        0.932377\n",
      "69        0.938819\n",
      "70        0.892457\n",
      "71        0.952171\n",
      "72        0.959208\n",
      "73        0.816947\n",
      "74        0.757436\n",
      "75        0.905142\n",
      "76        0.929351\n",
      "77        0.914909\n",
      "78        0.923215\n",
      "79        0.892232\n",
      "80        0.989566\n",
      "81        0.938651\n",
      "82        0.930512\n",
      "83        0.907814\n",
      "84        0.892569\n",
      "85        0.886969\n",
      "86        0.900843\n",
      "87        0.884655\n",
      "88        0.960224\n",
      "89        0.899043\n",
      "90        0.915862\n",
      "91        0.892849\n",
      "92        0.923697\n",
      "93        0.956488\n",
      "94        0.908875\n",
      "95        0.932359\n",
      "96        0.999944\n",
      "97        0.854345\n",
      "98        0.685003\n",
      "99        0.755108\n",
      "100       0.831220\n",
      "101       0.974495\n",
      "102       0.927899\n",
      "103       0.983120\n",
      "104       0.854345\n",
      "105       0.974814\n",
      "106       0.823654\n",
      "107       0.841630\n",
      "108       0.798598\n",
      "109       0.790545\n",
      "110       0.953806\n",
      "111       0.938482\n",
      "112       0.961215\n",
      "113       0.992366\n",
      "114       0.875689\n",
      "115       0.854457\n",
      "116       0.861593\n",
      "117       0.961552\n",
      "118       0.976707\n",
      "119       0.953808\n",
      "120       0.904487\n",
      "121       0.928258\n",
      "122       0.976226\n",
      "123       0.991918\n",
      "124       0.971834\n",
      "125       0.928677\n",
      "126       0.999776\n",
      "127       0.936671\n",
      "128       0.948112\n",
      "129       0.699273 \n",
      "\n",
      "Green Model Validation\n",
      "     Youdens Index\n",
      "0         0.923047\n",
      "1         0.898431\n",
      "2         0.874495\n",
      "3         0.938539\n",
      "4         0.984047\n",
      "5         0.976987\n",
      "6         0.950091\n",
      "7         0.914852\n",
      "8         0.900539\n",
      "9         0.903563\n",
      "10        0.976482\n",
      "11        0.907275\n",
      "12        0.914943\n",
      "13        0.953413\n",
      "14        0.938204\n",
      "15        0.991805\n",
      "16        0.907443\n",
      "17        0.883849\n",
      "18        0.862315\n",
      "19        0.815433\n",
      "20        0.934511\n",
      "21        0.838853\n",
      "22        0.992310\n",
      "23        0.984452\n",
      "24        0.861810\n",
      "25        0.870982\n",
      "26        0.976482\n",
      "27        0.933391\n",
      "28        0.927864\n",
      "29        0.976931\n",
      "30        0.999944\n",
      "31        0.999888\n",
      "32        0.825213\n",
      "33        0.930288\n",
      "34        0.823923\n",
      "35        0.908109\n",
      "36        0.918026\n",
      "37        0.827899\n",
      "38        0.903720\n",
      "39        0.747026\n",
      "40        0.919271\n",
      "41        0.864959\n",
      "42        0.695196\n",
      "43        0.953469\n",
      "44        0.976847\n",
      "45        0.892625\n",
      "46        0.961720\n",
      "47        0.992198\n",
      "48        0.992254\n",
      "49        0.968030\n",
      "50        0.991974\n",
      "51        0.945948\n",
      "52        0.930961\n",
      "53        0.975414\n",
      "54        0.938370\n",
      "55        0.884655\n",
      "56        0.934991\n",
      "57        0.904201\n",
      "58        0.892849\n",
      "59        0.892737\n",
      "60        0.992030\n",
      "61        0.946116\n",
      "62        0.838348\n",
      "63        0.946172\n",
      "64        0.922991\n",
      "65        0.937980\n",
      "66        0.851069\n",
      "67        0.930456\n",
      "68        0.932997\n",
      "69        0.861586\n",
      "70        0.914740\n",
      "71        0.913109\n",
      "72        0.901793\n",
      "73        0.838094\n",
      "74        0.848935\n",
      "75        0.836990\n",
      "76        0.992131\n",
      "77        0.900258\n",
      "78        0.992030\n",
      "79        0.876853\n",
      "80        0.989510\n",
      "81        0.907163\n",
      "82        0.885103\n",
      "83        0.870349\n",
      "84        0.915470\n",
      "85        0.902336\n",
      "86        0.900955\n",
      "87        0.938258\n",
      "88        0.846290\n",
      "89        0.835210\n",
      "90        0.961159\n",
      "91        0.915638\n",
      "92        0.872997\n",
      "93        0.886856\n",
      "94        0.914887\n",
      "95        0.943201\n",
      "96        0.969297\n",
      "97        0.824203\n",
      "98        0.800405\n",
      "99        0.777392\n",
      "100       0.823305\n",
      "101       0.865938\n",
      "102       0.910322\n",
      "103       0.970164\n",
      "104       0.884823\n",
      "105       0.991119\n",
      "106       0.898038\n",
      "107       0.883456\n",
      "108       0.832492\n",
      "109       0.832324\n",
      "110       0.915694\n",
      "111       0.930849\n",
      "112       0.693422\n",
      "113       0.999832\n",
      "114       0.914543\n",
      "115       0.876909\n",
      "116       0.820477\n",
      "117       0.938707\n",
      "118       0.976594\n",
      "119       0.973718\n",
      "120       0.937933\n",
      "121       0.943712\n",
      "122       0.945144\n",
      "123       0.953974\n",
      "124       0.955778\n",
      "125       0.939282\n",
      "126       0.901359\n",
      "127       0.936279\n",
      "128       0.959053\n",
      "129       0.793873 \n",
      "\n",
      "Blue Model Validation\n",
      "     Youdens Index\n",
      "0         0.877021\n",
      "1         0.965602\n",
      "2         0.787604\n",
      "3         0.900258\n",
      "4         0.976127\n",
      "5         0.961552\n",
      "6         0.925220\n",
      "7         0.861754\n",
      "8         0.922823\n",
      "9         0.877700\n",
      "10        0.938595\n",
      "11        0.884486\n",
      "12        0.852751\n",
      "13        0.938258\n",
      "14        0.920436\n",
      "15        0.960822\n",
      "16        0.899922\n",
      "17        0.817341\n",
      "18        0.853840\n",
      "19        0.840377\n",
      "20        0.909728\n",
      "21        0.838236\n",
      "22        0.930624\n",
      "23        0.946004\n",
      "24        0.823249\n",
      "25        0.854321\n",
      "26        0.861866\n",
      "27        0.907165\n",
      "28        0.881222\n",
      "29        0.945836\n",
      "30        1.000000\n",
      "31        0.961439\n",
      "32        0.817285\n",
      "33        0.868939\n",
      "34        0.792210\n",
      "35        0.790433\n",
      "36        0.910120\n",
      "37        0.751317\n",
      "38        0.935720\n",
      "39        0.808655\n",
      "40        0.879495\n",
      "41        0.897214\n",
      "42        0.758655\n",
      "43        0.923103\n",
      "44        0.938372\n",
      "45        0.777841\n",
      "46        0.976538\n",
      "47        0.961720\n",
      "48        0.899754\n",
      "49        0.920018\n",
      "50        0.992030\n",
      "51        0.915582\n",
      "52        0.899810\n",
      "53        0.975302\n",
      "54        0.984228\n",
      "55        0.900315\n",
      "56        0.927159\n",
      "57        0.863902\n",
      "58        0.922823\n",
      "59        0.953806\n",
      "60        0.953357\n",
      "61        0.869331\n",
      "62        0.892513\n",
      "63        0.869500\n",
      "64        0.961103\n",
      "65        0.937980\n",
      "66        0.801798\n",
      "67        0.815952\n",
      "68        0.887030\n",
      "69        0.900483\n",
      "70        0.815616\n",
      "71        0.905184\n",
      "72        0.930078\n",
      "73        0.831101\n",
      "74        0.890433\n",
      "75        0.785596\n",
      "76        0.983478\n",
      "77        0.915245\n",
      "78        0.953750\n",
      "79        0.915077\n",
      "80        0.999606\n",
      "81        0.854401\n",
      "82        0.900034\n",
      "83        0.853713\n",
      "84        0.800741\n",
      "85        0.940276\n",
      "86        0.863638\n",
      "87        0.846655\n",
      "88        0.846740\n",
      "89        0.924740\n",
      "90        0.861473\n",
      "91        0.953806\n",
      "92        0.772380\n",
      "93        0.843963\n",
      "94        0.813088\n",
      "95        0.790672\n",
      "96        0.984396\n",
      "97        0.785530\n",
      "98        0.708409\n",
      "99        0.700214\n",
      "100       0.762517\n",
      "101       0.790545\n",
      "102       0.955077\n",
      "103       0.949417\n",
      "104       0.899641\n",
      "105       0.958397\n",
      "106       0.848935\n",
      "107       0.900041\n",
      "108       0.782268\n",
      "109       0.815097\n",
      "110       0.845926\n",
      "111       0.884150\n",
      "112       0.992366\n",
      "113       0.992254\n",
      "114       0.914711\n",
      "115       0.823305\n",
      "116       0.841063\n",
      "117       0.922991\n",
      "118       0.953189\n",
      "119       0.963904\n",
      "120       0.882264\n",
      "121       0.932828\n",
      "122       0.937220\n",
      "123       0.953694\n",
      "124       0.967044\n",
      "125       0.939226\n",
      "126       0.860375\n",
      "127       0.896853\n",
      "128       0.989679\n",
      "129       0.754391 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Red Train\n",
    "dtc_red = DecisionTreeClassifier()\n",
    "dtc_red.fit(X_red_train, Y_train)\n",
    "vector_red = dtc_red.predict(X_red_train)\n",
    "\n",
    "# Green Train\n",
    "dtc_green = DecisionTreeClassifier()\n",
    "dtc_green.fit(X_green_train, Y_train)\n",
    "vector_green = dtc_green.predict(X_green_train)\n",
    "\n",
    "# Blue Train\n",
    "dtc_blue = DecisionTreeClassifier()\n",
    "dtc_blue.fit(X_blue_train, Y_train)\n",
    "vector_blue = dtc_blue.predict(X_blue_train)\n",
    "\n",
    "# Validation\n",
    "vector_red_val = dtc_red.predict(X_red_validation)\n",
    "vector_green_val = dtc_green.predict(X_green_validation)\n",
    "vector_blue_val = dtc_blue.predict(X_blue_validation)\n",
    "\n",
    "print(\"Red Model Validation\")\n",
    "print(get_youdens_index(vector_red_val, Y_validation), \"\\n\")\n",
    "\n",
    "print(\"Green Model Validation\")\n",
    "print(get_youdens_index(vector_green_val, Y_validation), \"\\n\")\n",
    "\n",
    "print(\"Blue Model Validation\")\n",
    "print(get_youdens_index(vector_blue_val, Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b11013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89       131\n",
      "           1       0.97      0.93      0.95       119\n",
      "           2       0.88      0.76      0.82       128\n",
      "           3       0.97      0.85      0.91       131\n",
      "           4       0.97      0.97      0.97       129\n",
      "           5       0.97      0.92      0.95       131\n",
      "           6       0.93      0.91      0.92       122\n",
      "           7       0.88      0.86      0.87       131\n",
      "           8       0.97      0.89      0.92       131\n",
      "           9       0.93      0.83      0.88       115\n",
      "          10       0.96      0.94      0.95       131\n",
      "          11       0.95      0.89      0.92       131\n",
      "          12       0.92      0.92      0.92       178\n",
      "          13       0.92      0.91      0.91       131\n",
      "          14       0.96      0.91      0.94       114\n",
      "          15       0.92      0.95      0.94       131\n",
      "          16       0.94      0.90      0.92       131\n",
      "          17       0.96      0.83      0.89       121\n",
      "          18       0.85      0.85      0.85       131\n",
      "          19       0.92      0.78      0.84       120\n",
      "          20       0.91      0.91      0.91       123\n",
      "          21       0.94      0.89      0.91       131\n",
      "          22       0.99      0.98      0.98       131\n",
      "          23       0.96      0.98      0.97       131\n",
      "          24       0.95      0.80      0.87       131\n",
      "          25       0.94      0.87      0.90       187\n",
      "          26       0.95      0.94      0.95       131\n",
      "          27       0.94      0.93      0.94       197\n",
      "          28       0.84      0.87      0.86       197\n",
      "          29       0.96      0.93      0.95       131\n",
      "          30       1.00      1.00      1.00       131\n",
      "          31       0.98      0.98      0.98       131\n",
      "          32       0.90      0.74      0.81       121\n",
      "          33       1.00      0.95      0.97       131\n",
      "          34       0.86      0.73      0.79       131\n",
      "          35       1.00      0.88      0.94       120\n",
      "          36       0.94      0.89      0.92       123\n",
      "          37       0.93      0.77      0.84       105\n",
      "          38       0.97      0.90      0.93       125\n",
      "          39       0.92      0.87      0.89       131\n",
      "          40       0.96      0.90      0.93       125\n",
      "          41       0.93      0.95      0.94       187\n",
      "          42       0.89      0.75      0.81        79\n",
      "          43       0.92      0.95      0.94       131\n",
      "          44       0.98      0.98      0.98       263\n",
      "          45       0.91      0.73      0.81       131\n",
      "          46       0.99      0.95      0.97       131\n",
      "          47       0.98      0.99      0.98       131\n",
      "          48       0.99      0.97      0.98       131\n",
      "          49       0.99      0.89      0.94       126\n",
      "          50       0.98      0.99      0.98       131\n",
      "          51       0.94      0.92      0.93       131\n",
      "          52       0.90      0.95      0.92       131\n",
      "          53       0.96      0.88      0.92       124\n",
      "          54       0.97      0.87      0.92       131\n",
      "          55       0.93      0.88      0.91       131\n",
      "          56       0.96      0.90      0.93       125\n",
      "          57       0.94      0.87      0.90       126\n",
      "          58       0.97      0.88      0.92       131\n",
      "          59       0.96      0.95      0.96       131\n",
      "          60       0.90      0.95      0.93       131\n",
      "          61       0.84      0.86      0.85       131\n",
      "          62       0.87      0.83      0.85       131\n",
      "          63       0.97      0.95      0.96       131\n",
      "          64       0.95      0.94      0.94       131\n",
      "          65       0.95      0.91      0.93       114\n",
      "          66       0.88      0.79      0.83        81\n",
      "          67       0.93      0.79      0.85       131\n",
      "          68       0.88      0.88      0.88       197\n",
      "          69       0.97      0.88      0.92       131\n",
      "          70       0.88      0.88      0.88       131\n",
      "          71       0.92      0.90      0.91       128\n",
      "          72       0.95      0.92      0.93       174\n",
      "          73       0.88      0.78      0.83       143\n",
      "          74       0.89      0.71      0.79       120\n",
      "          75       0.93      0.77      0.84       117\n",
      "          76       1.00      0.99      1.00       128\n",
      "          77       0.93      0.87      0.90       131\n",
      "          78       0.97      0.98      0.97       131\n",
      "          79       0.91      0.89      0.90       131\n",
      "          80       0.97      0.99      0.98       197\n",
      "          81       0.93      0.84      0.88       131\n",
      "          82       0.95      0.87      0.91       131\n",
      "          83       0.90      0.85      0.87       186\n",
      "          84       0.90      0.79      0.84       131\n",
      "          85       0.89      0.88      0.88       187\n",
      "          86       0.07      0.86      0.13        81\n",
      "          87       0.92      0.90      0.91       131\n",
      "          88       0.83      0.84      0.83       178\n",
      "          89       0.91      0.89      0.90       190\n",
      "          90       0.91      0.84      0.87       131\n",
      "          91       0.97      0.94      0.95       131\n",
      "          92       0.91      0.86      0.88       119\n",
      "          93       0.93      0.95      0.94       187\n",
      "          94       0.94      0.85      0.89       178\n",
      "          95       0.85      0.78      0.81       178\n",
      "          96       0.98      0.97      0.98       131\n",
      "          97       0.90      0.79      0.84       131\n",
      "          98       0.88      0.76      0.81       131\n",
      "          99       0.81      0.63      0.71       131\n",
      "         100       0.88      0.75      0.81       131\n",
      "         101       0.96      0.82      0.88       120\n",
      "         102       0.97      0.87      0.92       112\n",
      "         103       0.98      0.98      0.98       241\n",
      "         104       0.91      0.89      0.90       131\n",
      "         105       0.99      0.95      0.97       121\n",
      "         106       0.83      0.84      0.84       120\n",
      "         107       0.97      0.87      0.92       121\n",
      "         108       0.89      0.77      0.83       120\n",
      "         109       0.88      0.77      0.82       120\n",
      "         110       0.95      0.89      0.92       131\n",
      "         111       0.96      0.92      0.94       131\n",
      "         112       1.00      0.89      0.94       131\n",
      "         113       0.99      0.98      0.99       131\n",
      "         114       0.95      0.88      0.92       130\n",
      "         115       0.95      0.82      0.88       131\n",
      "         116       0.88      0.86      0.87       197\n",
      "         117       0.91      0.92      0.91       131\n",
      "         118       0.96      0.98      0.97       131\n",
      "         119       0.92      0.97      0.95       197\n",
      "         120       0.93      0.91      0.92       180\n",
      "         121       0.90      0.93      0.91       197\n",
      "         122       0.99      0.90      0.94       128\n",
      "         123       0.95      0.94      0.94       131\n",
      "         124       0.98      0.96      0.97       183\n",
      "         125       0.99      0.91      0.95        99\n",
      "         126       0.97      0.82      0.89       122\n",
      "         127       0.93      0.88      0.91       127\n",
      "         128       0.97      0.96      0.96       197\n",
      "         129       0.81      0.69      0.74       127\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     17956\n",
      "   macro avg       0.93      0.88      0.90     17956\n",
      "weighted avg       0.93      0.89      0.90     17956\n",
      " samples avg       0.89      0.89      0.89     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.862091\n",
      "1         0.932605\n",
      "2         0.757083\n",
      "3         0.854737\n",
      "4         0.968768\n",
      "5         0.923440\n",
      "6         0.909387\n",
      "7         0.861754\n",
      "8         0.885272\n",
      "9         0.834390\n",
      "10        0.938651\n",
      "11        0.892793\n",
      "12        0.920505\n",
      "13        0.907780\n",
      "14        0.912057\n",
      "15        0.953581\n",
      "16        0.900315\n",
      "17        0.826222\n",
      "18        0.853896\n",
      "19        0.774551\n",
      "20        0.909952\n",
      "21        0.892681\n",
      "22        0.977043\n",
      "23        0.976819\n",
      "24        0.801190\n",
      "25        0.865747\n",
      "26        0.938595\n",
      "27        0.933391\n",
      "28        0.871295\n",
      "29        0.931017\n",
      "30        1.000000\n",
      "31        0.984621\n",
      "32        0.743241\n",
      "33        0.946565\n",
      "34        0.731983\n",
      "35        0.883333\n",
      "36        0.893916\n",
      "37        0.771092\n",
      "38        0.895832\n",
      "39        0.869668\n",
      "40        0.895720\n",
      "41        0.945792\n",
      "42        0.746444\n",
      "43        0.953581\n",
      "44        0.976847\n",
      "45        0.724686\n",
      "46        0.946509\n",
      "47        0.992198\n",
      "48        0.969410\n",
      "49        0.888833\n",
      "50        0.992198\n",
      "51        0.915582\n",
      "52        0.945779\n",
      "53        0.878752\n",
      "54        0.870005\n",
      "55        0.877414\n",
      "56        0.903720\n",
      "57        0.864687\n",
      "58        0.877694\n",
      "59        0.953918\n",
      "60        0.953413\n",
      "61        0.861417\n",
      "62        0.831107\n",
      "63        0.946340\n",
      "64        0.938539\n",
      "65        0.912000\n",
      "66        0.789620\n",
      "67        0.785811\n",
      "68        0.881954\n",
      "69        0.877694\n",
      "70        0.877021\n",
      "71        0.897877\n",
      "72        0.919034\n",
      "73        0.782319\n",
      "74        0.707717\n",
      "75        0.768838\n",
      "76        0.992188\n",
      "77        0.869780\n",
      "78        0.976875\n",
      "79        0.884823\n",
      "80        0.989510\n",
      "81        0.839246\n",
      "82        0.869892\n",
      "83        0.848449\n",
      "84        0.793220\n",
      "85        0.875824\n",
      "86        0.812953\n",
      "87        0.900202\n",
      "88        0.835335\n",
      "89        0.893780\n",
      "90        0.839078\n",
      "91        0.938707\n",
      "92        0.856582\n",
      "93        0.945792\n",
      "94        0.847752\n",
      "95        0.773931\n",
      "96        0.969353\n",
      "97        0.785586\n",
      "98        0.754996\n",
      "99        0.632522\n",
      "100       0.747362\n",
      "101       0.816442\n",
      "102       0.865903\n",
      "103       0.983120\n",
      "104       0.884879\n",
      "105       0.950357\n",
      "106       0.840545\n",
      "107       0.867600\n",
      "108       0.766050\n",
      "109       0.765938\n",
      "110       0.892793\n",
      "111       0.923384\n",
      "112       0.893130\n",
      "113       0.984677\n",
      "114       0.884279\n",
      "115       0.824091\n",
      "116       0.861593\n",
      "117       0.915357\n",
      "118       0.976819\n",
      "119       0.973718\n",
      "120       0.904880\n",
      "121       0.927752\n",
      "122       0.898381\n",
      "123       0.938539\n",
      "124       0.956115\n",
      "125       0.909035\n",
      "126       0.819504\n",
      "127       0.881441\n",
      "128       0.959053\n",
      "129       0.683918 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stack the rgb predictions to get combi model values \n",
    "X_combined_train = np.column_stack((vector_red, vector_green, vector_blue)) # shape (1745, 18) - 18 features\n",
    "X_combined_val = np.column_stack((vector_red_val, vector_green_val, vector_blue_val)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Combi Train\n",
    "dtc_combi =  DecisionTreeClassifier()\n",
    "dtc_combi.fit(X_combined_train, Y_train)\n",
    "\n",
    "# Validation\n",
    "print(\"Combined Model Validation\")\n",
    "combi_val = dtc_combi.predict(X_combined_val)\n",
    "print(classification_report(Y_validation, combi_val, zero_division=0))\n",
    "print(get_youdens_index(combi_val, Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a20193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91       131\n",
      "           1       0.97      0.95      0.96       118\n",
      "           2       0.91      0.88      0.89       128\n",
      "           3       0.95      0.87      0.91       131\n",
      "           4       0.97      0.91      0.94       128\n",
      "           5       0.93      0.92      0.92       132\n",
      "           6       0.92      0.87      0.89       122\n",
      "           7       0.90      0.85      0.88       131\n",
      "           8       0.93      0.94      0.94       131\n",
      "           9       0.94      0.84      0.89       115\n",
      "          10       0.97      0.92      0.94       131\n",
      "          11       0.90      0.90      0.90       131\n",
      "          12       0.96      0.92      0.94       179\n",
      "          13       0.85      0.89      0.87       131\n",
      "          14       0.97      0.93      0.95       114\n",
      "          15       0.92      0.90      0.91       132\n",
      "          16       0.86      0.90      0.88       131\n",
      "          17       0.99      0.78      0.87       121\n",
      "          18       0.89      0.92      0.90       131\n",
      "          19       0.85      0.77      0.81       120\n",
      "          20       0.94      0.82      0.87       123\n",
      "          21       0.95      0.89      0.92       131\n",
      "          22       1.00      0.95      0.98       131\n",
      "          23       0.97      0.98      0.98       131\n",
      "          24       0.95      0.85      0.90       131\n",
      "          25       0.89      0.83      0.86       187\n",
      "          26       0.97      0.92      0.94       131\n",
      "          27       0.97      0.96      0.97       197\n",
      "          28       0.80      0.84      0.82       197\n",
      "          29       0.95      0.94      0.95       131\n",
      "          30       1.00      0.99      1.00       131\n",
      "          31       0.99      0.99      0.99       132\n",
      "          32       0.91      0.76      0.83       121\n",
      "          33       1.00      0.92      0.96       131\n",
      "          34       0.90      0.79      0.84       131\n",
      "          35       0.98      0.82      0.89       120\n",
      "          36       0.95      0.88      0.91       123\n",
      "          37       0.97      0.84      0.90       104\n",
      "          38       0.96      0.86      0.91       125\n",
      "          39       0.87      0.84      0.85       131\n",
      "          40       0.95      0.93      0.94       125\n",
      "          41       0.95      0.96      0.95       187\n",
      "          42       0.93      0.70      0.80        79\n",
      "          43       0.95      0.94      0.94       131\n",
      "          44       0.97      0.97      0.97       262\n",
      "          45       0.93      0.88      0.91       131\n",
      "          46       1.00      0.96      0.98       131\n",
      "          47       0.99      1.00      1.00       131\n",
      "          48       0.98      0.97      0.98       131\n",
      "          49       0.98      0.94      0.96       126\n",
      "          50       0.97      0.90      0.93       131\n",
      "          51       0.95      0.89      0.92       131\n",
      "          52       0.97      0.95      0.96       131\n",
      "          53       0.96      0.91      0.93       124\n",
      "          54       0.94      0.95      0.94       131\n",
      "          55       0.94      0.82      0.88       131\n",
      "          56       0.95      0.91      0.93       124\n",
      "          57       0.92      0.91      0.92       126\n",
      "          58       0.94      0.89      0.91       131\n",
      "          59       0.93      0.95      0.94       131\n",
      "          60       0.91      0.92      0.92       131\n",
      "          61       0.89      0.85      0.87       131\n",
      "          62       0.85      0.85      0.85       131\n",
      "          63       0.96      0.90      0.93       131\n",
      "          64       0.98      0.90      0.94       131\n",
      "          65       0.97      0.89      0.93       114\n",
      "          66       0.86      0.88      0.87        80\n",
      "          67       0.90      0.85      0.87       131\n",
      "          68       0.88      0.89      0.89       197\n",
      "          69       0.99      0.89      0.94       131\n",
      "          70       0.90      0.80      0.85       131\n",
      "          71       0.94      0.81      0.87       128\n",
      "          72       0.93      0.88      0.91       175\n",
      "          73       0.84      0.80      0.82       142\n",
      "          74       0.92      0.68      0.78       120\n",
      "          75       0.96      0.63      0.76       117\n",
      "          76       0.99      0.96      0.98       128\n",
      "          77       0.95      0.91      0.93       131\n",
      "          78       0.98      0.95      0.96       131\n",
      "          79       0.92      0.89      0.90       131\n",
      "          80       0.94      0.98      0.96       197\n",
      "          81       0.92      0.82      0.87       131\n",
      "          82       0.92      0.92      0.92       131\n",
      "          83       0.92      0.88      0.90       186\n",
      "          84       0.93      0.85      0.88       131\n",
      "          85       0.92      0.93      0.92       188\n",
      "          86       0.07      0.90      0.13        80\n",
      "          87       0.95      0.88      0.91       131\n",
      "          88       0.86      0.85      0.86       178\n",
      "          89       0.91      0.93      0.92       190\n",
      "          90       0.88      0.86      0.87       132\n",
      "          91       0.98      0.92      0.95       132\n",
      "          92       0.92      0.85      0.88       118\n",
      "          93       0.95      0.97      0.96       187\n",
      "          94       0.96      0.85      0.90       178\n",
      "          95       0.85      0.79      0.82       178\n",
      "          96       0.99      0.98      0.98       131\n",
      "          97       0.83      0.81      0.82       131\n",
      "          98       0.83      0.69      0.75       131\n",
      "          99       0.77      0.58      0.66       132\n",
      "         100       0.87      0.75      0.80       131\n",
      "         101       0.89      0.74      0.81       120\n",
      "         102       1.00      0.89      0.94       113\n",
      "         103       1.00      0.97      0.98       241\n",
      "         104       0.89      0.82      0.85       131\n",
      "         105       0.99      0.96      0.97       121\n",
      "         106       0.88      0.86      0.87       120\n",
      "         107       0.95      0.79      0.86       121\n",
      "         108       0.90      0.79      0.84       120\n",
      "         109       0.89      0.68      0.77       120\n",
      "         110       0.97      0.91      0.94       131\n",
      "         111       0.94      0.95      0.94       131\n",
      "         112       1.00      0.89      0.94       131\n",
      "         113       0.99      0.98      0.99       131\n",
      "         114       0.95      0.89      0.92       131\n",
      "         115       0.92      0.85      0.88       131\n",
      "         116       0.89      0.93      0.91       197\n",
      "         117       0.91      0.91      0.91       131\n",
      "         118       0.97      0.95      0.96       131\n",
      "         119       0.97      0.97      0.97       197\n",
      "         120       0.93      0.92      0.93       179\n",
      "         121       0.93      0.93      0.93       197\n",
      "         122       0.98      0.95      0.97       128\n",
      "         123       0.96      0.98      0.97       131\n",
      "         124       0.92      0.96      0.94       183\n",
      "         125       0.97      0.91      0.94        99\n",
      "         126       0.94      0.87      0.90       123\n",
      "         127       0.97      0.83      0.90       126\n",
      "         128       0.97      0.94      0.96       197\n",
      "         129       0.89      0.77      0.83       126\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     17956\n",
      "   macro avg       0.93      0.88      0.90     17956\n",
      "weighted avg       0.93      0.89      0.90     17956\n",
      " samples avg       0.89      0.89      0.89     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.877526\n",
      "1         0.948984\n",
      "2         0.874383\n",
      "3         0.869892\n",
      "4         0.913894\n",
      "5         0.916162\n",
      "6         0.868348\n",
      "7         0.854233\n",
      "8         0.938426\n",
      "9         0.843142\n",
      "10        0.915806\n",
      "11        0.900034\n",
      "12        0.921394\n",
      "13        0.884318\n",
      "14        0.929656\n",
      "15        0.900954\n",
      "16        0.899697\n",
      "17        0.776803\n",
      "18        0.915189\n",
      "19        0.765770\n",
      "20        0.820746\n",
      "21        0.885160\n",
      "22        0.954198\n",
      "23        0.984508\n",
      "24        0.854625\n",
      "25        0.827808\n",
      "26        0.915806\n",
      "27        0.964129\n",
      "28        0.840275\n",
      "29        0.938595\n",
      "30        0.992366\n",
      "31        0.992368\n",
      "32        0.759826\n",
      "33        0.923664\n",
      "34        0.785586\n",
      "35        0.816555\n",
      "36        0.877712\n",
      "37        0.836370\n",
      "38        0.863776\n",
      "39        0.838741\n",
      "40        0.927664\n",
      "41        0.956656\n",
      "42        0.695979\n",
      "43        0.938539\n",
      "44        0.968957\n",
      "45        0.877414\n",
      "46        0.961832\n",
      "47        0.999944\n",
      "48        0.969353\n",
      "49        0.944332\n",
      "50        0.900539\n",
      "51        0.892793\n",
      "52        0.953974\n",
      "53        0.911010\n",
      "54        0.946116\n",
      "55        0.824035\n",
      "56        0.910954\n",
      "57        0.912138\n",
      "58        0.885103\n",
      "59        0.946060\n",
      "60        0.922991\n",
      "61        0.846543\n",
      "62        0.846206\n",
      "63        0.900483\n",
      "64        0.900595\n",
      "65        0.894569\n",
      "66        0.874385\n",
      "67        0.846655\n",
      "68        0.887030\n",
      "69        0.893074\n",
      "70        0.800854\n",
      "71        0.812107\n",
      "72        0.879381\n",
      "73        0.801582\n",
      "74        0.682941\n",
      "75        0.632310\n",
      "76        0.960881\n",
      "77        0.908060\n",
      "78        0.946397\n",
      "79        0.884935\n",
      "80        0.984096\n",
      "81        0.823923\n",
      "82        0.915413\n",
      "83        0.875556\n",
      "84        0.846823\n",
      "85        0.929951\n",
      "86        0.848590\n",
      "87        0.877526\n",
      "88        0.846965\n",
      "89        0.925303\n",
      "90        0.862795\n",
      "91        0.916498\n",
      "92        0.846953\n",
      "93        0.967352\n",
      "94        0.847977\n",
      "95        0.790785\n",
      "96        0.977043\n",
      "97        0.807926\n",
      "98        0.685957\n",
      "99        0.574467\n",
      "100       0.747250\n",
      "101       0.741050\n",
      "102       0.893805\n",
      "103       0.970898\n",
      "104       0.816065\n",
      "105       0.958622\n",
      "106       0.857548\n",
      "107       0.793108\n",
      "108       0.791106\n",
      "109       0.682773\n",
      "110       0.908173\n",
      "111       0.946116\n",
      "112       0.893130\n",
      "113       0.984677\n",
      "114       0.885160\n",
      "115       0.846767\n",
      "116       0.927695\n",
      "117       0.907724\n",
      "118       0.953974\n",
      "119       0.969262\n",
      "120       0.921113\n",
      "121       0.933278\n",
      "122       0.953013\n",
      "123       0.976763\n",
      "124       0.960905\n",
      "125       0.908923\n",
      "126       0.869526\n",
      "127       0.833165\n",
      "128       0.943881\n",
      "129       0.769168 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_red_test = dtc_red.predict(X_red_test)\n",
    "vector_green_test = dtc_green.predict(X_green_test)\n",
    "vector_blue_test = dtc_blue.predict(X_blue_test)\n",
    "\n",
    "X_combined_test = np.column_stack((vector_red_test, vector_green_test, vector_blue_test)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Test Final\n",
    "print(\"Combined Model Test\")\n",
    "combi_test = dtc_combi.predict(X_combined_test)\n",
    "print(classification_report(Y_test, combi_test, zero_division=0))\n",
    "print(get_youdens_index(combi_test, Y_test), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
