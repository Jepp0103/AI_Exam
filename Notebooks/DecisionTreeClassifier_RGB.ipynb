{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1c9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# Path to image folders, \n",
    "#data_path = fr'../Datasets/config'\n",
    "data_path = fr'../Datasets/full/Training'\n",
    "\n",
    "def remove_white_background(pixels):\n",
    "    newPixels = []\n",
    "    for pixel in pixels:\n",
    "        pixel = list(pixel)\n",
    "        if ((256 > pixel[0] > 200) and (256 > pixel[1] > 200) and (256 > pixel[2] > 200)):\n",
    "            pixel[0] = 0\n",
    "            pixel[1] = 0\n",
    "            pixel[2] = 0\n",
    "        newPixels.append(pixel)\n",
    "    \n",
    "    return newPixels\n",
    "\n",
    "\n",
    "def redify(pixels):\n",
    "    return [r for r, g, b in pixels]\n",
    "\n",
    "                \n",
    "def greenify(pixels):\n",
    "    return [g for r, g, b in pixels]\n",
    "\n",
    "\n",
    "def blueify(pixels):\n",
    "    return [b for r, g, b in pixels]\n",
    "\n",
    "\n",
    "def get_rgb_pixels_onehot_labels(src):\n",
    "    print(\"Starting...\")\n",
    "    newPixels = []\n",
    "    y = np.empty(shape=[0, 1])\n",
    "\n",
    "    for subdir in os.listdir(src):\n",
    "        current_path = os.path.join(src, subdir)\n",
    "        for file in os.listdir(current_path):\n",
    "            img = Image.open(os.path.join(current_path, file))\n",
    "            imgResize = img.resize((24,24))\n",
    "            pixels = list(imgResize.getdata())\n",
    "            pixels = remove_white_background(pixels)\n",
    "            newPixels.append(pixels)\n",
    "            y = np.append(y, subdir)\n",
    "    return newPixels, LabelBinarizer().fit_transform(y) # OneHot encode y\n",
    "\n",
    "\n",
    "def process_files(src):\n",
    "    X_red_train = []\n",
    "    X_red_validation = []\n",
    "    X_red_test = []\n",
    "    X_green_train = []\n",
    "    X_green_validation = []\n",
    "    X_green_test = []\n",
    "    X_blue_train = []\n",
    "    X_blue_validation = []\n",
    "    X_blue_test = []\n",
    "    all_pixels, y = get_rgb_pixels_onehot_labels(src)\n",
    "\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(all_pixels, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "    \n",
    "    for pixels in X_train:       \n",
    "        X_red_train.append(redify(pixels.copy()))\n",
    "        X_green_train.append(greenify(pixels.copy()))\n",
    "        X_blue_train.append(blueify(pixels.copy()))\n",
    "        \n",
    "    for pixels in X_validation:       \n",
    "        X_red_validation.append(redify(pixels.copy()))\n",
    "        X_green_validation.append(greenify(pixels.copy()))\n",
    "        X_blue_validation.append(blueify(pixels.copy()))\n",
    "        \n",
    "    for pixels in X_test:       \n",
    "        X_red_test.append(redify(pixels.copy()))\n",
    "        X_green_test.append(greenify(pixels.copy()))\n",
    "        X_blue_test.append(blueify(pixels.copy()))\n",
    "    \n",
    "    \n",
    "    print(\"Finished \\n\")\n",
    "    return np.asarray(X_red_train), np.asarray(X_red_validation), np.asarray(X_red_test), np.asarray(X_green_train), np.asarray(X_green_validation), np.asarray(X_green_test), np.asarray(X_blue_train), np.asarray(X_blue_validation), np.asarray(X_blue_test), y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "def get_youdens_index(predictions, Y):\n",
    "    # Calculate true positive/negative and false positive/negative\n",
    "    tp = sum((Y == predictions) * (Y == 1) * 1)\n",
    "    tn = sum((Y == predictions) * (Y == 0) * 1)\n",
    "    fp = sum((Y != predictions) * (Y == 0) * 1)\n",
    "    fn = sum((Y != predictions) * (Y == 1) * 1)\n",
    "    \n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    result = sensitivity - (1 - specificity)\n",
    "    # Put it in a dateframe for nicer visuals\n",
    "    df = pd.DataFrame({'Youdens Index': result})\n",
    "    pd.set_option('display.max_rows', 200)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69d3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Finished \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call process_files and assign variables\n",
    "X_red_train, X_red_validation, X_red_test, X_green_train, X_green_validation, X_green_test, X_blue_train, X_blue_validation, X_blue_test, Y_train, Y_validation, Y_test = process_files(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5feb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Model Validation\n",
      "     Youdens Index\n",
      "0         0.945892\n",
      "1         0.948963\n",
      "2         0.858758\n",
      "3         0.930568\n",
      "4         0.906248\n",
      "5         0.923215\n",
      "6         0.966148\n",
      "7         0.899641\n",
      "8         0.961383\n",
      "9         0.894643\n",
      "10        0.884486\n",
      "11        0.960991\n",
      "12        0.953987\n",
      "13        0.930063\n",
      "14        0.964800\n",
      "15        0.938595\n",
      "16        0.900258\n",
      "17        0.867600\n",
      "18        0.876404\n",
      "19        0.757436\n",
      "20        0.983347\n",
      "21        0.923215\n",
      "22        1.000000\n",
      "23        0.938539\n",
      "24        0.877358\n",
      "25        0.811990\n",
      "26        0.991974\n",
      "27        0.907672\n",
      "28        0.974394\n",
      "29        0.984733\n",
      "30        0.984677\n",
      "31        0.984621\n",
      "32        0.776131\n",
      "33        0.976931\n",
      "34        0.853728\n",
      "35        0.932829\n",
      "36        0.910064\n",
      "37        0.856807\n",
      "38        0.911215\n",
      "39        0.861642\n",
      "40        0.919607\n",
      "41        0.945961\n",
      "42        0.847486\n",
      "43        0.938370\n",
      "44        0.992056\n",
      "45        0.854233\n",
      "46        0.999888\n",
      "47        0.984733\n",
      "48        0.999944\n",
      "49        0.960205\n",
      "50        0.992254\n",
      "51        0.999944\n",
      "52        0.930400\n",
      "53        0.870183\n",
      "54        0.938539\n",
      "55        0.961495\n",
      "56        0.887271\n",
      "57        0.888216\n",
      "58        0.915750\n",
      "59        0.961495\n",
      "60        0.992198\n",
      "61        0.884711\n",
      "62        0.907780\n",
      "63        0.938651\n",
      "64        0.915694\n",
      "65        0.911832\n",
      "66        0.777218\n",
      "67        0.930961\n",
      "68        0.896732\n",
      "69        0.946060\n",
      "70        0.892232\n",
      "71        0.943798\n",
      "72        0.941910\n",
      "73        0.802456\n",
      "74        0.773430\n",
      "75        0.905310\n",
      "76        0.913614\n",
      "77        0.922318\n",
      "78        0.930681\n",
      "79        0.838180\n",
      "80        0.999887\n",
      "81        0.923159\n",
      "82        0.922935\n",
      "83        0.902325\n",
      "84        0.869444\n",
      "85        0.892035\n",
      "86        0.913356\n",
      "87        0.854345\n",
      "88        0.960055\n",
      "89        0.940698\n",
      "90        0.915694\n",
      "91        0.930961\n",
      "92        0.915686\n",
      "93        0.929750\n",
      "94        0.908594\n",
      "95        0.943201\n",
      "96        0.999944\n",
      "97        0.877133\n",
      "98        0.730973\n",
      "99        0.770431\n",
      "100       0.792939\n",
      "101       0.966162\n",
      "102       0.919307\n",
      "103       0.991363\n",
      "104       0.884935\n",
      "105       0.941924\n",
      "106       0.857212\n",
      "107       0.841966\n",
      "108       0.782100\n",
      "109       0.740153\n",
      "110       0.908285\n",
      "111       0.915750\n",
      "112       0.961327\n",
      "113       0.992366\n",
      "114       0.883718\n",
      "115       0.854681\n",
      "116       0.881785\n",
      "117       0.953806\n",
      "118       0.968849\n",
      "119       0.964073\n",
      "120       0.915654\n",
      "121       0.933278\n",
      "122       0.968357\n",
      "123       0.999719\n",
      "124       0.966707\n",
      "125       0.898374\n",
      "126       0.983326\n",
      "127       0.920811\n",
      "128       0.943318\n",
      "129       0.746517 \n",
      "\n",
      "Green Model Validation\n",
      "     Youdens Index\n",
      "0         0.923047\n",
      "1         0.915406\n",
      "2         0.850665\n",
      "3         0.953750\n",
      "4         0.960623\n",
      "5         0.953918\n",
      "6         0.884573\n",
      "7         0.861305\n",
      "8         0.907836\n",
      "9         0.894867\n",
      "10        0.961552\n",
      "11        0.892232\n",
      "12        0.914605\n",
      "13        0.922823\n",
      "14        0.946864\n",
      "15        0.984228\n",
      "16        0.930400\n",
      "17        0.916963\n",
      "18        0.862034\n",
      "19        0.765209\n",
      "20        0.934230\n",
      "21        0.869612\n",
      "22        0.999888\n",
      "23        0.976819\n",
      "24        0.869668\n",
      "25        0.860062\n",
      "26        0.961215\n",
      "27        0.923295\n",
      "28        0.922394\n",
      "29        0.984452\n",
      "30        1.000000\n",
      "31        0.999888\n",
      "32        0.866759\n",
      "33        0.923103\n",
      "34        0.839190\n",
      "35        0.899664\n",
      "36        0.910064\n",
      "37        0.799216\n",
      "38        0.927439\n",
      "39        0.746577\n",
      "40        0.919159\n",
      "41        0.870251\n",
      "42        0.707798\n",
      "43        0.938258\n",
      "44        0.977017\n",
      "45        0.884711\n",
      "46        0.961664\n",
      "47        0.992198\n",
      "48        0.999888\n",
      "49        0.967861\n",
      "50        0.992086\n",
      "51        0.953750\n",
      "52        0.922823\n",
      "53        0.975302\n",
      "54        0.923271\n",
      "55        0.869387\n",
      "56        0.927551\n",
      "57        0.895984\n",
      "58        0.915862\n",
      "59        0.908060\n",
      "60        0.976763\n",
      "61        0.969241\n",
      "62        0.860968\n",
      "63        0.938202\n",
      "64        0.923215\n",
      "65        0.929264\n",
      "66        0.863302\n",
      "67        0.938034\n",
      "68        0.922732\n",
      "69        0.831220\n",
      "70        0.899810\n",
      "71        0.881747\n",
      "72        0.901512\n",
      "73        0.837533\n",
      "74        0.824439\n",
      "75        0.879333\n",
      "76        0.992075\n",
      "77        0.915526\n",
      "78        0.984228\n",
      "79        0.884374\n",
      "80        0.989454\n",
      "81        0.884262\n",
      "82        0.907780\n",
      "83        0.854163\n",
      "84        0.915470\n",
      "85        0.913257\n",
      "86        0.913133\n",
      "87        0.907499\n",
      "88        0.824268\n",
      "89        0.840248\n",
      "90        0.922935\n",
      "91        0.907499\n",
      "92        0.856694\n",
      "93        0.897101\n",
      "94        0.931909\n",
      "95        0.931459\n",
      "96        0.954142\n",
      "97        0.854513\n",
      "98        0.739055\n",
      "99        0.785418\n",
      "100       0.785530\n",
      "101       0.824159\n",
      "102       0.910266\n",
      "103       0.965902\n",
      "104       0.861866\n",
      "105       0.982742\n",
      "106       0.840433\n",
      "107       0.874800\n",
      "108       0.823598\n",
      "109       0.865601\n",
      "110       0.885440\n",
      "111       0.938426\n",
      "112       0.678043\n",
      "113       0.999944\n",
      "114       0.953005\n",
      "115       0.839190\n",
      "116       0.815175\n",
      "117       0.984565\n",
      "118       0.969073\n",
      "119       0.974338\n",
      "120       0.937651\n",
      "121       0.918275\n",
      "122       0.937388\n",
      "123       0.984677\n",
      "124       0.972453\n",
      "125       0.979630\n",
      "126       0.917584\n",
      "127       0.928405\n",
      "128       0.933672\n",
      "129       0.793649 \n",
      "\n",
      "Blue Model Validation\n",
      "     Youdens Index\n",
      "0         0.908004\n",
      "1         0.948627\n",
      "2         0.787772\n",
      "3         0.915189\n",
      "4         0.983879\n",
      "5         0.953806\n",
      "6         0.917416\n",
      "7         0.869387\n",
      "8         0.930793\n",
      "9         0.886452\n",
      "10        0.922823\n",
      "11        0.868826\n",
      "12        0.881179\n",
      "13        0.914909\n",
      "14        0.928816\n",
      "15        0.938202\n",
      "16        0.861866\n",
      "17        0.833365\n",
      "18        0.853728\n",
      "19        0.848374\n",
      "20        0.893748\n",
      "21        0.823137\n",
      "22        0.915077\n",
      "23        0.938314\n",
      "24        0.815952\n",
      "25        0.854264\n",
      "26        0.823586\n",
      "27        0.881897\n",
      "28        0.866106\n",
      "29        0.946060\n",
      "30        1.000000\n",
      "31        0.969129\n",
      "32        0.858046\n",
      "33        0.900146\n",
      "34        0.784689\n",
      "35        0.824439\n",
      "36        0.893972\n",
      "37        0.808684\n",
      "38        0.935439\n",
      "39        0.815672\n",
      "40        0.855047\n",
      "41        0.913369\n",
      "42        0.720456\n",
      "43        0.915357\n",
      "44        0.945977\n",
      "45        0.762461\n",
      "46        0.953806\n",
      "47        0.946284\n",
      "48        0.899641\n",
      "49        0.936228\n",
      "50        0.984565\n",
      "51        0.922935\n",
      "52        0.923047\n",
      "53        0.991767\n",
      "54        0.946116\n",
      "55        0.907836\n",
      "56        0.927215\n",
      "57        0.863845\n",
      "58        0.953413\n",
      "59        0.984172\n",
      "60        0.961159\n",
      "61        0.900034\n",
      "62        0.869219\n",
      "63        0.892232\n",
      "64        0.961103\n",
      "65        0.929208\n",
      "66        0.814032\n",
      "67        0.831051\n",
      "68        0.872027\n",
      "69        0.915526\n",
      "70        0.861530\n",
      "71        0.882139\n",
      "72        0.930078\n",
      "73        0.817115\n",
      "74        0.865994\n",
      "75        0.759282\n",
      "76        0.976170\n",
      "77        0.900090\n",
      "78        0.938426\n",
      "79        0.930400\n",
      "80        0.974394\n",
      "81        0.884879\n",
      "82        0.876965\n",
      "83        0.832545\n",
      "84        0.838853\n",
      "85        0.956263\n",
      "86        0.875928\n",
      "87        0.831276\n",
      "88        0.830223\n",
      "89        0.887785\n",
      "90        0.838853\n",
      "91        0.915526\n",
      "92        0.755293\n",
      "93        0.827977\n",
      "94        0.807301\n",
      "95        0.785279\n",
      "96        0.976875\n",
      "97        0.793164\n",
      "98        0.776775\n",
      "99        0.723452\n",
      "100       0.747474\n",
      "101       0.815489\n",
      "102       0.936996\n",
      "103       0.961019\n",
      "104       0.892569\n",
      "105       0.941981\n",
      "106       0.849047\n",
      "107       0.908362\n",
      "108       0.815433\n",
      "109       0.815097\n",
      "110       0.815335\n",
      "111       0.861642\n",
      "112       0.992366\n",
      "113       0.992310\n",
      "114       0.914655\n",
      "115       0.823025\n",
      "116       0.835424\n",
      "117       0.938258\n",
      "118       0.945667\n",
      "119       0.963791\n",
      "120       0.887651\n",
      "121       0.912410\n",
      "122       0.952845\n",
      "123       0.946004\n",
      "124       0.966932\n",
      "125       0.959204\n",
      "126       0.860151\n",
      "127       0.896796\n",
      "128       0.979470\n",
      "129       0.738419 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Red Train\n",
    "dtc_red = DecisionTreeClassifier()\n",
    "dtc_red.fit(X_red_train, Y_train)\n",
    "vector_red = dtc_red.predict(X_red_train)\n",
    "\n",
    "# Green Train\n",
    "dtc_green = DecisionTreeClassifier()\n",
    "dtc_green.fit(X_green_train, Y_train)\n",
    "vector_green = dtc_green.predict(X_green_train)\n",
    "\n",
    "# Blue Train\n",
    "dtc_blue = DecisionTreeClassifier()\n",
    "dtc_blue.fit(X_blue_train, Y_train)\n",
    "vector_blue = dtc_blue.predict(X_blue_train)\n",
    "\n",
    "# Validation\n",
    "vector_red_val = dtc_red.predict(X_red_validation)\n",
    "vector_green_val = dtc_green.predict(X_green_validation)\n",
    "vector_blue_val = dtc_blue.predict(X_blue_validation)\n",
    "\n",
    "print(\"Red Model Validation\")\n",
    "print(get_youdens_index(vector_red_val, Y_validation), \"\\n\")\n",
    "\n",
    "print(\"Green Model Validation\")\n",
    "print(get_youdens_index(vector_green_val, Y_validation), \"\\n\")\n",
    "\n",
    "print(\"Blue Model Validation\")\n",
    "print(get_youdens_index(vector_blue_val, Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c812af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.88      0.92       131\n",
      "           1       0.96      0.92      0.94       119\n",
      "           2       0.94      0.73      0.82       128\n",
      "           3       0.98      0.92      0.94       131\n",
      "           4       0.96      0.95      0.96       129\n",
      "           5       0.98      0.95      0.96       131\n",
      "           6       0.99      0.87      0.93       122\n",
      "           7       0.88      0.85      0.87       131\n",
      "           8       0.94      0.91      0.93       131\n",
      "           9       0.93      0.87      0.90       115\n",
      "          10       0.98      0.93      0.96       131\n",
      "          11       0.90      0.92      0.91       131\n",
      "          12       0.93      0.87      0.90       178\n",
      "          13       0.91      0.90      0.90       131\n",
      "          14       0.99      0.91      0.95       114\n",
      "          15       1.00      0.97      0.98       131\n",
      "          16       0.94      0.90      0.92       131\n",
      "          17       0.97      0.88      0.93       121\n",
      "          18       0.93      0.85      0.89       131\n",
      "          19       0.81      0.76      0.78       120\n",
      "          20       0.98      0.93      0.95       123\n",
      "          21       0.95      0.89      0.92       131\n",
      "          22       1.00      1.00      1.00       131\n",
      "          23       0.97      0.98      0.97       131\n",
      "          24       0.94      0.83      0.88       131\n",
      "          25       0.92      0.81      0.86       187\n",
      "          26       0.96      0.97      0.97       131\n",
      "          27       0.95      0.92      0.94       197\n",
      "          28       0.88      0.92      0.90       197\n",
      "          29       0.98      0.98      0.98       131\n",
      "          30       0.99      0.98      0.99       131\n",
      "          31       0.98      0.98      0.98       131\n",
      "          32       0.94      0.74      0.83       121\n",
      "          33       0.98      0.98      0.98       131\n",
      "          34       0.95      0.81      0.88       131\n",
      "          35       0.96      0.93      0.94       120\n",
      "          36       0.99      0.89      0.94       123\n",
      "          37       0.91      0.76      0.83       105\n",
      "          38       0.91      0.86      0.88       125\n",
      "          39       0.85      0.79      0.82       131\n",
      "          40       0.90      0.84      0.87       125\n",
      "          41       0.87      0.87      0.87       187\n",
      "          42       0.90      0.76      0.82        79\n",
      "          43       0.94      0.92      0.93       131\n",
      "          44       0.95      0.95      0.95       263\n",
      "          45       0.95      0.79      0.86       131\n",
      "          46       0.98      0.96      0.97       131\n",
      "          47       0.96      0.95      0.95       131\n",
      "          48       0.98      0.97      0.98       131\n",
      "          49       0.98      0.95      0.96       126\n",
      "          50       0.98      0.98      0.98       131\n",
      "          51       0.94      0.93      0.93       131\n",
      "          52       0.94      0.90      0.92       131\n",
      "          53       0.98      0.97      0.98       124\n",
      "          54       0.95      0.92      0.94       131\n",
      "          55       0.90      0.85      0.88       131\n",
      "          56       0.94      0.88      0.91       125\n",
      "          57       0.94      0.82      0.87       126\n",
      "          58       0.97      0.89      0.93       131\n",
      "          59       0.96      0.95      0.96       131\n",
      "          60       0.98      0.96      0.97       131\n",
      "          61       0.97      0.88      0.92       131\n",
      "          62       0.90      0.82      0.86       131\n",
      "          63       0.97      0.94      0.95       131\n",
      "          64       0.95      0.92      0.93       131\n",
      "          65       0.95      0.89      0.92       114\n",
      "          66       0.06      0.83      0.11        81\n",
      "          67       0.92      0.79      0.85       131\n",
      "          68       0.90      0.87      0.88       197\n",
      "          69       0.92      0.82      0.87       131\n",
      "          70       0.89      0.88      0.88       131\n",
      "          71       0.88      0.84      0.86       128\n",
      "          72       0.93      0.89      0.91       174\n",
      "          73       0.88      0.78      0.83       143\n",
      "          74       0.80      0.71      0.75       120\n",
      "          75       0.91      0.80      0.85       117\n",
      "          76       0.97      0.94      0.95       128\n",
      "          77       0.91      0.86      0.89       131\n",
      "          78       0.96      0.92      0.94       131\n",
      "          79       0.84      0.83      0.84       131\n",
      "          80       0.98      0.97      0.98       197\n",
      "          81       0.93      0.91      0.92       131\n",
      "          82       0.88      0.87      0.87       131\n",
      "          83       0.92      0.83      0.87       186\n",
      "          84       0.89      0.83      0.86       131\n",
      "          85       0.89      0.90      0.90       187\n",
      "          86       0.99      0.88      0.93        81\n",
      "          87       0.89      0.83      0.86       131\n",
      "          88       0.91      0.83      0.87       178\n",
      "          89       0.88      0.94      0.91       190\n",
      "          90       0.93      0.80      0.86       131\n",
      "          91       0.98      0.89      0.93       131\n",
      "          92       0.99      0.75      0.85       119\n",
      "          93       0.93      0.91      0.92       187\n",
      "          94       0.90      0.79      0.84       178\n",
      "          95       0.92      0.93      0.92       178\n",
      "          96       1.00      0.95      0.98       131\n",
      "          97       0.90      0.79      0.84       131\n",
      "          98       0.77      0.76      0.77       131\n",
      "          99       0.93      0.62      0.74       131\n",
      "         100       0.91      0.78      0.84       131\n",
      "         101       0.91      0.78      0.84       120\n",
      "         102       0.94      0.89      0.92       112\n",
      "         103       0.94      0.97      0.95       241\n",
      "         104       0.94      0.83      0.88       131\n",
      "         105       0.98      0.93      0.95       121\n",
      "         106       0.88      0.80      0.84       120\n",
      "         107       0.94      0.89      0.92       121\n",
      "         108       0.87      0.75      0.81       120\n",
      "         109       0.95      0.72      0.82       120\n",
      "         110       0.91      0.80      0.85       131\n",
      "         111       0.97      0.87      0.92       131\n",
      "         112       0.94      0.88      0.91       131\n",
      "         113       1.00      0.99      1.00       131\n",
      "         114       0.94      0.92      0.93       130\n",
      "         115       0.86      0.76      0.81       131\n",
      "         116       0.82      0.83      0.82       197\n",
      "         117       0.98      0.97      0.97       131\n",
      "         118       0.93      0.96      0.95       131\n",
      "         119       0.97      0.97      0.97       197\n",
      "         120       0.92      0.88      0.90       180\n",
      "         121       0.96      0.92      0.94       197\n",
      "         122       0.98      0.92      0.95       128\n",
      "         123       0.98      1.00      0.99       131\n",
      "         124       0.98      0.97      0.98       183\n",
      "         125       0.95      0.88      0.91        99\n",
      "         126       0.96      0.86      0.91       122\n",
      "         127       0.95      0.91      0.93       127\n",
      "         128       0.98      0.98      0.98       197\n",
      "         129       0.81      0.69      0.74       127\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     17956\n",
      "   macro avg       0.93      0.88      0.90     17956\n",
      "weighted avg       0.93      0.88      0.90     17956\n",
      " samples avg       0.88      0.88      0.88     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.877694\n",
      "1         0.924089\n",
      "2         0.726226\n",
      "3         0.915862\n",
      "4         0.953208\n",
      "5         0.946397\n",
      "6         0.868796\n",
      "7         0.854120\n",
      "8         0.908004\n",
      "9         0.869117\n",
      "10        0.931186\n",
      "11        0.915301\n",
      "12        0.864494\n",
      "13        0.900090\n",
      "14        0.912225\n",
      "15        0.969466\n",
      "16        0.900371\n",
      "17        0.884129\n",
      "18        0.846879\n",
      "19        0.757100\n",
      "20        0.926717\n",
      "21        0.892793\n",
      "22        1.000000\n",
      "23        0.976875\n",
      "24        0.831668\n",
      "25        0.812046\n",
      "26        0.969185\n",
      "27        0.923295\n",
      "28        0.922450\n",
      "29        0.984621\n",
      "30        0.984677\n",
      "31        0.984621\n",
      "32        0.743465\n",
      "33        0.976931\n",
      "34        0.808880\n",
      "35        0.924720\n",
      "36        0.886123\n",
      "37        0.761457\n",
      "38        0.855439\n",
      "39        0.785250\n",
      "40        0.839327\n",
      "41        0.864903\n",
      "42        0.759102\n",
      "43        0.915638\n",
      "44        0.945977\n",
      "45        0.793557\n",
      "46        0.961720\n",
      "47        0.946284\n",
      "48        0.969353\n",
      "49        0.952213\n",
      "50        0.984621\n",
      "51        0.930849\n",
      "52        0.900371\n",
      "53        0.967630\n",
      "54        0.923328\n",
      "55        0.854289\n",
      "56        0.879607\n",
      "57        0.817068\n",
      "58        0.885328\n",
      "59        0.953918\n",
      "60        0.961720\n",
      "61        0.877638\n",
      "62        0.823754\n",
      "63        0.938707\n",
      "64        0.915694\n",
      "65        0.894457\n",
      "66        0.770042\n",
      "67        0.785755\n",
      "68        0.866950\n",
      "69        0.823866\n",
      "70        0.877077\n",
      "71        0.842909\n",
      "72        0.890186\n",
      "73        0.782319\n",
      "74        0.707156\n",
      "75        0.802914\n",
      "76        0.937276\n",
      "77        0.861978\n",
      "78        0.915750\n",
      "79        0.830883\n",
      "80        0.974394\n",
      "81        0.907892\n",
      "82        0.869331\n",
      "83        0.827225\n",
      "84        0.831332\n",
      "85        0.902561\n",
      "86        0.876487\n",
      "87        0.831332\n",
      "88        0.825055\n",
      "89        0.940698\n",
      "90        0.801078\n",
      "91        0.885384\n",
      "92        0.747843\n",
      "93        0.913763\n",
      "94        0.785617\n",
      "95        0.926179\n",
      "96        0.954198\n",
      "97        0.793220\n",
      "98        0.761676\n",
      "99        0.617984\n",
      "100       0.778065\n",
      "101       0.774495\n",
      "102       0.892521\n",
      "103       0.965902\n",
      "104       0.831668\n",
      "105       0.925508\n",
      "106       0.799271\n",
      "107       0.892169\n",
      "108       0.749271\n",
      "109       0.716386\n",
      "110       0.800910\n",
      "111       0.870005\n",
      "112       0.877470\n",
      "113       0.992366\n",
      "114       0.922628\n",
      "115       0.762461\n",
      "116       0.825384\n",
      "117       0.969297\n",
      "118       0.961327\n",
      "119       0.974338\n",
      "120       0.876990\n",
      "121       0.918331\n",
      "122       0.921763\n",
      "123       0.999832\n",
      "124       0.972453\n",
      "125       0.878508\n",
      "126       0.860431\n",
      "127       0.905175\n",
      "128       0.979470\n",
      "129       0.683862 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stack the rgb predictions to get combi model values \n",
    "X_combined_train = np.column_stack((vector_red, vector_green, vector_blue)) # shape (1745, 18) - 18 features\n",
    "X_combined_val = np.column_stack((vector_red_val, vector_green_val, vector_blue_val)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Combi Train\n",
    "dtc_combi =  DecisionTreeClassifier()\n",
    "dtc_combi.fit(X_combined_train, Y_train)\n",
    "\n",
    "# Validation\n",
    "print(\"Combined Model Validation\")\n",
    "combi_val = dtc_combi.predict(X_combined_val)\n",
    "print(classification_report(Y_validation, combi_val, zero_division=0))\n",
    "print(get_youdens_index(combi_val, Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667f91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.92       131\n",
      "           1       0.96      0.97      0.96       118\n",
      "           2       0.94      0.79      0.86       128\n",
      "           3       0.99      0.95      0.97       131\n",
      "           4       0.95      0.89      0.92       128\n",
      "           5       0.95      0.92      0.93       132\n",
      "           6       0.95      0.87      0.91       122\n",
      "           7       0.88      0.88      0.88       131\n",
      "           8       0.95      0.92      0.94       131\n",
      "           9       0.96      0.88      0.92       115\n",
      "          10       0.95      0.92      0.94       131\n",
      "          11       0.94      0.85      0.90       131\n",
      "          12       0.90      0.91      0.90       179\n",
      "          13       0.90      0.88      0.89       131\n",
      "          14       0.98      0.87      0.92       114\n",
      "          15       0.97      0.95      0.96       132\n",
      "          16       0.94      0.90      0.92       131\n",
      "          17       1.00      0.80      0.89       121\n",
      "          18       0.83      0.77      0.80       131\n",
      "          19       0.86      0.80      0.83       120\n",
      "          20       0.98      0.93      0.96       123\n",
      "          21       0.96      0.86      0.91       131\n",
      "          22       1.00      1.00      1.00       131\n",
      "          23       0.95      0.97      0.96       131\n",
      "          24       0.94      0.85      0.90       131\n",
      "          25       0.89      0.80      0.84       187\n",
      "          26       0.95      0.87      0.91       131\n",
      "          27       0.96      0.96      0.96       197\n",
      "          28       0.89      0.87      0.88       197\n",
      "          29       0.98      0.95      0.96       131\n",
      "          30       0.99      0.98      0.99       131\n",
      "          31       0.98      0.97      0.97       132\n",
      "          32       0.90      0.69      0.78       121\n",
      "          33       0.98      0.96      0.97       131\n",
      "          34       0.93      0.84      0.88       131\n",
      "          35       0.99      0.83      0.90       120\n",
      "          36       0.98      0.85      0.91       123\n",
      "          37       0.93      0.74      0.82       104\n",
      "          38       0.95      0.89      0.92       125\n",
      "          39       0.93      0.82      0.87       131\n",
      "          40       0.91      0.85      0.88       125\n",
      "          41       0.88      0.85      0.87       187\n",
      "          42       0.95      0.66      0.78        79\n",
      "          43       0.98      0.95      0.97       131\n",
      "          44       0.95      0.96      0.96       262\n",
      "          45       0.97      0.91      0.94       131\n",
      "          46       0.98      0.95      0.96       131\n",
      "          47       0.98      0.94      0.96       131\n",
      "          48       0.98      0.92      0.95       131\n",
      "          49       0.98      0.96      0.97       126\n",
      "          50       0.94      0.89      0.91       131\n",
      "          51       0.93      0.91      0.92       131\n",
      "          52       0.93      0.88      0.91       131\n",
      "          53       0.98      0.97      0.98       124\n",
      "          54       0.94      0.90      0.92       131\n",
      "          55       0.89      0.87      0.88       131\n",
      "          56       0.95      0.91      0.93       124\n",
      "          57       0.95      0.87      0.91       126\n",
      "          58       0.94      0.90      0.92       131\n",
      "          59       0.96      0.94      0.95       131\n",
      "          60       0.95      0.94      0.94       131\n",
      "          61       0.93      0.83      0.88       131\n",
      "          62       0.90      0.85      0.88       131\n",
      "          63       0.96      0.95      0.95       131\n",
      "          64       0.98      0.95      0.96       131\n",
      "          65       0.98      0.89      0.93       114\n",
      "          66       0.06      0.84      0.12        80\n",
      "          67       0.93      0.87      0.90       131\n",
      "          68       0.92      0.86      0.89       197\n",
      "          69       0.92      0.82      0.87       131\n",
      "          70       0.87      0.85      0.86       131\n",
      "          71       0.88      0.83      0.85       128\n",
      "          72       0.88      0.86      0.87       175\n",
      "          73       0.88      0.82      0.85       142\n",
      "          74       0.78      0.68      0.73       120\n",
      "          75       0.97      0.72      0.82       117\n",
      "          76       0.95      0.95      0.95       128\n",
      "          77       0.93      0.89      0.91       131\n",
      "          78       0.94      0.97      0.95       131\n",
      "          79       0.88      0.84      0.86       131\n",
      "          80       0.99      0.98      0.99       197\n",
      "          81       0.89      0.87      0.88       131\n",
      "          82       0.94      0.89      0.91       131\n",
      "          83       0.92      0.88      0.90       186\n",
      "          84       0.90      0.86      0.88       131\n",
      "          85       0.89      0.94      0.91       188\n",
      "          86       0.97      0.86      0.91        80\n",
      "          87       0.94      0.87      0.90       131\n",
      "          88       0.91      0.88      0.89       178\n",
      "          89       0.89      0.90      0.89       190\n",
      "          90       0.94      0.86      0.90       132\n",
      "          91       0.98      0.93      0.96       132\n",
      "          92       0.96      0.75      0.84       118\n",
      "          93       0.94      0.94      0.94       187\n",
      "          94       0.91      0.82      0.86       178\n",
      "          95       0.92      0.89      0.90       178\n",
      "          96       1.00      0.97      0.98       131\n",
      "          97       0.85      0.80      0.83       131\n",
      "          98       0.81      0.68      0.74       131\n",
      "          99       0.94      0.71      0.81       132\n",
      "         100       0.95      0.79      0.86       131\n",
      "         101       0.93      0.75      0.83       120\n",
      "         102       0.98      0.88      0.93       113\n",
      "         103       0.92      0.93      0.92       241\n",
      "         104       0.90      0.87      0.89       131\n",
      "         105       0.99      0.94      0.97       121\n",
      "         106       0.83      0.77      0.80       120\n",
      "         107       0.84      0.84      0.84       121\n",
      "         108       0.87      0.79      0.83       120\n",
      "         109       0.94      0.62      0.74       120\n",
      "         110       0.87      0.85      0.86       131\n",
      "         111       0.97      0.85      0.91       131\n",
      "         112       0.93      0.87      0.90       131\n",
      "         113       0.99      0.97      0.98       131\n",
      "         114       0.95      0.90      0.93       131\n",
      "         115       0.89      0.80      0.84       131\n",
      "         116       0.81      0.82      0.82       197\n",
      "         117       0.98      0.98      0.98       131\n",
      "         118       0.93      0.95      0.94       131\n",
      "         119       0.98      0.96      0.97       197\n",
      "         120       0.92      0.89      0.91       179\n",
      "         121       0.95      0.93      0.94       197\n",
      "         122       0.98      0.98      0.98       128\n",
      "         123       0.98      0.95      0.96       131\n",
      "         124       0.95      0.95      0.95       183\n",
      "         125       0.97      0.93      0.95        99\n",
      "         126       0.96      0.89      0.92       123\n",
      "         127       0.92      0.87      0.89       126\n",
      "         128       0.97      0.97      0.97       197\n",
      "         129       0.84      0.75      0.79       126\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     17956\n",
      "   macro avg       0.93      0.88      0.90     17956\n",
      "weighted avg       0.93      0.88      0.90     17956\n",
      " samples avg       0.88      0.88      0.88     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.885272\n",
      "1         0.965821\n",
      "2         0.788726\n",
      "3         0.946509\n",
      "4         0.890288\n",
      "5         0.916330\n",
      "6         0.868572\n",
      "7         0.877021\n",
      "8         0.923328\n",
      "9         0.878037\n",
      "10        0.923328\n",
      "11        0.854569\n",
      "12        0.909546\n",
      "13        0.877133\n",
      "14        0.868309\n",
      "15        0.946745\n",
      "16        0.900315\n",
      "17        0.801653\n",
      "18        0.769814\n",
      "19        0.799103\n",
      "20        0.934847\n",
      "21        0.862315\n",
      "22        1.000000\n",
      "23        0.969073\n",
      "24        0.854569\n",
      "25        0.795722\n",
      "26        0.869892\n",
      "27        0.958940\n",
      "28        0.871858\n",
      "29        0.946397\n",
      "30        0.984677\n",
      "31        0.969529\n",
      "32        0.685446\n",
      "33        0.961664\n",
      "34        0.839246\n",
      "35        0.833277\n",
      "36        0.853546\n",
      "37        0.740049\n",
      "38        0.887664\n",
      "39        0.816345\n",
      "40        0.847383\n",
      "41        0.849086\n",
      "42        0.658060\n",
      "43        0.954030\n",
      "44        0.957337\n",
      "45        0.908173\n",
      "46        0.946453\n",
      "47        0.938819\n",
      "48        0.923496\n",
      "49        0.960149\n",
      "50        0.892681\n",
      "51        0.907892\n",
      "52        0.877414\n",
      "53        0.967630\n",
      "54        0.900315\n",
      "55        0.869444\n",
      "56        0.910954\n",
      "57        0.872679\n",
      "58        0.900315\n",
      "59        0.938651\n",
      "60        0.938539\n",
      "61        0.831612\n",
      "62        0.854233\n",
      "63        0.946284\n",
      "64        0.946397\n",
      "65        0.885853\n",
      "66        0.780720\n",
      "67        0.869780\n",
      "68        0.862100\n",
      "69        0.823866\n",
      "70        0.854008\n",
      "71        0.827284\n",
      "72        0.855962\n",
      "73        0.823045\n",
      "74        0.682044\n",
      "75        0.717781\n",
      "76        0.952732\n",
      "77        0.884991\n",
      "78        0.969017\n",
      "79        0.838853\n",
      "80        0.984659\n",
      "81        0.869444\n",
      "82        0.892681\n",
      "83        0.875500\n",
      "84        0.861922\n",
      "85        0.940195\n",
      "86        0.862388\n",
      "87        0.869836\n",
      "88        0.875561\n",
      "89        0.898762\n",
      "90        0.855668\n",
      "91        0.931706\n",
      "92        0.745538\n",
      "93        0.935154\n",
      "94        0.819437\n",
      "95        0.886853\n",
      "96        0.969466\n",
      "97        0.800517\n",
      "98        0.678211\n",
      "99        0.711785\n",
      "100       0.793557\n",
      "101       0.749608\n",
      "102       0.884844\n",
      "103       0.924182\n",
      "104       0.869556\n",
      "105       0.942093\n",
      "106       0.765601\n",
      "107       0.841910\n",
      "108       0.790882\n",
      "109       0.616386\n",
      "110       0.846375\n",
      "111       0.847160\n",
      "112       0.869724\n",
      "113       0.969410\n",
      "114       0.900427\n",
      "115       0.800797\n",
      "116       0.820195\n",
      "117       0.976931\n",
      "118       0.953694\n",
      "119       0.959166\n",
      "120       0.893067\n",
      "121       0.933447\n",
      "122       0.976450\n",
      "123       0.946453\n",
      "124       0.944849\n",
      "125       0.929125\n",
      "126       0.885955\n",
      "127       0.872455\n",
      "128       0.969262\n",
      "129       0.752959 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_red_test = dtc_red.predict(X_red_test)\n",
    "vector_green_test = dtc_green.predict(X_green_test)\n",
    "vector_blue_test = dtc_blue.predict(X_blue_test)\n",
    "\n",
    "X_combined_test = np.column_stack((vector_red_test, vector_green_test, vector_blue_test)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Test Final\n",
    "print(\"Combined Model Test\")\n",
    "combi_test = dtc_combi.predict(X_combined_test)\n",
    "print(classification_report(Y_test, combi_test, zero_division=0))\n",
    "print(get_youdens_index(combi_test, Y_test), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
