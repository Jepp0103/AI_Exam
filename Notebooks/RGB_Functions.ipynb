{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a39bdcf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "# Path to image folders, \n",
    "data_path = fr'../Datasets/config'\n",
    "\n",
    "def remove_white_background(pixels):\n",
    "    newPixels = []\n",
    "    for pixel in pixels:\n",
    "        pixel = list(pixel)\n",
    "        if ((256 > pixel[0] > 200) and (256 > pixel[1] > 200) and (256 > pixel[2] > 200)):\n",
    "            pixel[0] = 0\n",
    "            pixel[1] = 0\n",
    "            pixel[2] = 0\n",
    "        newPixels.append(pixel)\n",
    "    \n",
    "    return newPixels\n",
    "\n",
    "\n",
    "def redify(pixels):\n",
    "    return [r for r, g, b in pixels]\n",
    "\n",
    "                \n",
    "def greenify(pixels):\n",
    "    return [g for r, g, b in pixels]\n",
    "\n",
    "\n",
    "def blueify(pixels):\n",
    "    return [b for r, g, b in pixels]\n",
    "\n",
    "\n",
    "def get_rgb_pixels_onehot_labels(src):\n",
    "    print(\"Starting...\")\n",
    "    newPixels = []\n",
    "    y = np.empty(shape=[0, 1])\n",
    "\n",
    "    for subdir in os.listdir(src):\n",
    "        current_path = os.path.join(src, subdir)\n",
    "        for file in os.listdir(current_path):\n",
    "            img = Image.open(os.path.join(current_path, file))\n",
    "            imgResize = img.resize((24,24))\n",
    "            pixels = list(imgResize.getdata())\n",
    "            pixels = remove_white_background(pixels)\n",
    "            newPixels.append(pixels)\n",
    "            y = np.append(y, subdir)\n",
    "    return newPixels, LabelBinarizer().fit_transform(y) # OneHot encode y\n",
    "\n",
    "\n",
    "def process_files(src):\n",
    "    X_red_train = []\n",
    "    X_red_validation = []\n",
    "    X_red_test = []\n",
    "    X_green_train = []\n",
    "    X_green_validation = []\n",
    "    X_green_test = []\n",
    "    X_blue_train = []\n",
    "    X_blue_validation = []\n",
    "    X_blue_test = []\n",
    "    all_pixels, y = get_rgb_pixels_onehot_labels(src)\n",
    "\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(all_pixels, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "    \n",
    "    for pixels in X_train:       \n",
    "        X_red_train.append(redify(pixels.copy()))\n",
    "        X_green_train.append(greenify(pixels.copy()))\n",
    "        X_blue_train.append(blueify(pixels.copy()))\n",
    "        \n",
    "    for pixels in X_validation:       \n",
    "        X_red_validation.append(redify(pixels.copy()))\n",
    "        X_green_validation.append(greenify(pixels.copy()))\n",
    "        X_blue_validation.append(blueify(pixels.copy()))\n",
    "        \n",
    "    for pixels in X_test:       \n",
    "        X_red_test.append(redify(pixels.copy()))\n",
    "        X_green_test.append(greenify(pixels.copy()))\n",
    "        X_blue_test.append(blueify(pixels.copy()))\n",
    "    \n",
    "    \n",
    "    print(\"Finished \\n\")\n",
    "    return np.asarray(X_red_train), np.asarray(X_red_validation), np.asarray(X_red_test), np.asarray(X_green_train), np.asarray(X_green_validation), np.asarray(X_green_test), np.asarray(X_blue_train), np.asarray(X_blue_validation), np.asarray(X_blue_test), y_train, y_validation, y_test\n",
    "\n",
    "\n",
    "def get_youdens_index(predictions, Y):\n",
    "    # Calculate true positive/negative and false positive/negative\n",
    "    tp = sum((Y == predictions) * (Y == 1) * 1)\n",
    "    tn = sum((Y == predictions) * (Y == 0) * 1)\n",
    "    fp = sum((Y != predictions) * (Y == 0) * 1)\n",
    "    fn = sum((Y != predictions) * (Y == 1) * 1)\n",
    "    \n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    result = sensitivity - (1 - specificity)\n",
    "    # Put it in a dateframe for nicer visuals\n",
    "    df = pd.DataFrame({'Youdens Index': result})\n",
    "    pd.set_option('display.max_rows', 200)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61a32382",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Finished \n",
      "\n",
      "Red Config Tests\n",
      "Hidden layers:  432\n",
      "Score:  0.9862542955326461 Time:  2.572378396987915\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       0.977109\n",
      "2       0.989899\n",
      "3       1.000000\n",
      "4       1.000000\n",
      "5       0.990805 \n",
      "\n",
      "Hidden layers:  216\n",
      "Score:  0.9707903780068728 Time:  1.0659687519073486\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       0.968750\n",
      "2       0.987829\n",
      "3       0.941276\n",
      "4       0.995918\n",
      "5       0.990805 \n",
      "\n",
      "Hidden layers:  (216, 108)\n",
      "Score:  0.9810996563573883 Time:  1.4192888736724854\n",
      "   Youdens Index\n",
      "0       0.998028\n",
      "1       0.979167\n",
      "2       0.979798\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       0.986207 \n",
      "\n",
      "Hidden layers:  108\n",
      "Score:  0.9570446735395189 Time:  0.552501916885376\n",
      "   Youdens Index\n",
      "0       0.980750\n",
      "1       0.958462\n",
      "2       0.965556\n",
      "3       0.994106\n",
      "4       0.997959\n",
      "5       0.981703 \n",
      "\n",
      "Hidden layers:  (108, 54)\n",
      "Score:  0.9621993127147767 Time:  0.8077337741851807\n",
      "   Youdens Index\n",
      "0       0.889389\n",
      "1       0.989583\n",
      "2       0.985758\n",
      "3       0.994106\n",
      "4       1.000000\n",
      "5       0.970303 \n",
      "\n",
      "Green Config Tests\n",
      "Hidden layers:  432\n",
      "Score:  0.9879725085910653 Time:  1.7756128311157227\n",
      "   Youdens Index\n",
      "0       0.984694\n",
      "1       0.989583\n",
      "2       1.000000\n",
      "3       0.956939\n",
      "4       1.000000\n",
      "5       0.997701 \n",
      "\n",
      "Hidden layers:  216\n",
      "Score:  0.979381443298969 Time:  0.6555955410003662\n",
      "   Youdens Index\n",
      "0       0.958028\n",
      "1       0.989583\n",
      "2       0.975657\n",
      "3       0.996071\n",
      "4       1.000000\n",
      "5       0.997701 \n",
      "\n",
      "Hidden layers:  (216, 108)\n",
      "Score:  0.9742268041237113 Time:  0.7138502597808838\n",
      "   Youdens Index\n",
      "0       0.944694\n",
      "1       0.966692\n",
      "2       1.000000\n",
      "3       0.980407\n",
      "4       1.000000\n",
      "5       0.986395 \n",
      "\n",
      "Hidden layers:  108\n",
      "Score:  0.9725085910652921 Time:  0.374340295791626\n",
      "   Youdens Index\n",
      "0       0.929389\n",
      "1       0.983410\n",
      "2       0.977728\n",
      "3       0.982372\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Hidden layers:  (108, 54)\n",
      "Score:  0.9828178694158075 Time:  0.552311897277832\n",
      "   Youdens Index\n",
      "0       0.982722\n",
      "1       0.977109\n",
      "2       0.987829\n",
      "3       0.986301\n",
      "4       0.997959\n",
      "5       0.997701 \n",
      "\n",
      "Blue Config Tests\n",
      "Hidden layers:  432\n",
      "Score:  0.9862542955326461 Time:  1.454089641571045\n",
      "   Youdens Index\n",
      "0       0.973333\n",
      "1       0.983410\n",
      "2       0.989899\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Hidden layers:  216\n",
      "Score:  0.979381443298969 Time:  0.41737914085388184\n",
      "   Youdens Index\n",
      "0       0.998028\n",
      "1       0.977109\n",
      "2       0.981617\n",
      "3       0.984337\n",
      "4       1.000000\n",
      "5       0.995402 \n",
      "\n",
      "Hidden layers:  (216, 108)\n",
      "Score:  0.9862542955326461 Time:  0.6606001853942871\n",
      "   Youdens Index\n",
      "0       0.956055\n",
      "1       0.997942\n",
      "2       0.989899\n",
      "3       1.000000\n",
      "4       0.989130\n",
      "5       0.997701 \n",
      "\n",
      "Hidden layers:  108\n",
      "Score:  0.9621993127147767 Time:  0.3222928047180176\n",
      "   Youdens Index\n",
      "0       0.853333\n",
      "1       0.985468\n",
      "2       0.965556\n",
      "3       0.970638\n",
      "4       1.000000\n",
      "5       0.997701 \n",
      "\n",
      "Hidden layers:  (108, 54)\n",
      "Score:  0.9759450171821306 Time:  0.42438721656799316\n",
      "   Youdens Index\n",
      "0       0.982722\n",
      "1       0.995885\n",
      "2       0.959596\n",
      "3       0.958904\n",
      "4       1.000000\n",
      "5       0.988506 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Call process_files and assign variables\n",
    "X_red_train, X_red_validation, X_red_test, X_green_train, X_green_validation, X_green_test, X_blue_train, X_blue_validation, X_blue_test, Y_train, Y_validation, Y_test = process_files(data_path)\n",
    "# Fit/train train-datasets and store prediction vectors in variables\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Red Config HL Test\n",
    "print(\"Red Config HL Tests\")\n",
    "for hl in [(432), (216), (216, 108), (108), (108, 54)]:\n",
    "    print(\"Hidden layers: \", hl)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hl)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_red_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_red_validation, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_red_validation), Y_validation), \"\\n\")\n",
    "\n",
    "# Green Config HL Test\n",
    "print(\"Green Config HL Tests\")\n",
    "for hl in [(432), (216), (216, 108), (108), (108, 54)]:\n",
    "    print(\"Hidden layers: \", hl)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hl)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_green_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_green_validation, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_green_validation), Y_validation), \"\\n\")\n",
    "    \n",
    "# Blue Config HL Test\n",
    "print(\"Blue Config HL Tests\")\n",
    "for hl in [(432), (216), (216, 108), (108), (108, 54)]:\n",
    "    print(\"Hidden layers: \", hl)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hl)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_blue_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_blue_validation, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_blue_validation), Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa6d07e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Config Alpha Tests\n",
      "Alpha:  0.0001\n",
      "Score:  0.9604810996563574 Time:  0.788548469543457\n",
      "   Youdens Index\n",
      "0       0.960000\n",
      "1       0.941744\n",
      "2       0.947425\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       0.981609 \n",
      "\n",
      "Alpha:  0.001\n",
      "Score:  0.9690721649484536 Time:  0.6332933902740479\n",
      "   Youdens Index\n",
      "0       0.944694\n",
      "1       0.962577\n",
      "2       0.977728\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       0.986301 \n",
      "\n",
      "Alpha:  0.01\n",
      "Score:  0.9673539518900344 Time:  0.9388542175292969\n",
      "   Youdens Index\n",
      "0       0.942722\n",
      "1       0.952160\n",
      "2       0.987829\n",
      "3       1.000000\n",
      "4       1.000000\n",
      "5       0.986301 \n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9570446735395189 Time:  0.9438574314117432\n",
      "   Youdens Index\n",
      "0       0.944694\n",
      "1       0.968750\n",
      "2       0.951314\n",
      "3       0.968673\n",
      "4       1.000000\n",
      "5       0.970396 \n",
      "\n",
      "Alpha:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\olive\\documents\\6. semester\\applied artificial intelligence\\ai_exam\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9621993127147767 Time:  3.197904348373413\n",
      "   Youdens Index\n",
      "0       0.982722\n",
      "1       0.975051\n",
      "2       0.987578\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       0.972601 \n",
      "\n",
      "Green Config Alpha Tests\n",
      "Alpha:  0.0001\n",
      "Score:  0.9759450171821306 Time:  0.3292996883392334\n",
      "   Youdens Index\n",
      "0       0.940750\n",
      "1       0.989583\n",
      "2       0.987829\n",
      "3       0.982372\n",
      "4       0.997959\n",
      "5       0.990898 \n",
      "\n",
      "Alpha:  0.001\n",
      "Score:  0.9828178694158075 Time:  0.3813464641571045\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       0.991770\n",
      "2       0.977728\n",
      "3       0.982372\n",
      "4       1.000000\n",
      "5       0.997701 \n",
      "\n",
      "Alpha:  0.01\n",
      "Score:  0.9673539518900344 Time:  0.4353961944580078\n",
      "   Youdens Index\n",
      "0       0.898777\n",
      "1       0.968750\n",
      "2       0.987829\n",
      "3       0.982372\n",
      "4       0.997959\n",
      "5       0.993197 \n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9656357388316151 Time:  0.35732603073120117\n",
      "   Youdens Index\n",
      "0       0.946667\n",
      "1       0.995885\n",
      "2       0.953385\n",
      "3       0.964744\n",
      "4       1.000000\n",
      "5       0.990898 \n",
      "\n",
      "Alpha:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\olive\\documents\\6. semester\\applied artificial intelligence\\ai_exam\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9828178694158075 Time:  3.417895555496216\n",
      "   Youdens Index\n",
      "0       0.973333\n",
      "1       0.987526\n",
      "2       0.997930\n",
      "3       0.964744\n",
      "4       1.000000\n",
      "5       0.993103 \n",
      "\n",
      "Blue Config Alpha Tests\n",
      "Alpha:  0.0001\n",
      "Score:  0.9776632302405498 Time:  0.28500962257385254\n",
      "   Youdens Index\n",
      "0       0.931361\n",
      "1       0.995885\n",
      "2       0.983688\n",
      "3       0.996071\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Alpha:  0.001\n",
      "Score:  0.9759450171821306 Time:  0.27710485458374023\n",
      "   Youdens Index\n",
      "0       0.980750\n",
      "1       1.000000\n",
      "2       0.977728\n",
      "3       0.943241\n",
      "4       0.997959\n",
      "5       0.995402 \n",
      "\n",
      "Alpha:  0.01\n",
      "Score:  0.9725085910652921 Time:  0.3039891719818115\n",
      "   Youdens Index\n",
      "0       0.973333\n",
      "1       0.975051\n",
      "2       0.997930\n",
      "3       0.964744\n",
      "4       0.987090\n",
      "5       0.995402 \n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9570446735395189 Time:  0.3343040943145752\n",
      "   Youdens Index\n",
      "0       0.946667\n",
      "1       0.971065\n",
      "2       0.961415\n",
      "3       0.984337\n",
      "4       1.000000\n",
      "5       0.990898 \n",
      "\n",
      "Alpha:  1\n",
      "Score:  0.979381443298969 Time:  3.4496355056762695\n",
      "   Youdens Index\n",
      "0       0.904694\n",
      "1       1.000000\n",
      "2       0.997930\n",
      "3       0.970638\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\olive\\documents\\6. semester\\applied artificial intelligence\\ai_exam\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# default alpha=0.0001\n",
    "\n",
    "print(\"Red Config Alpha Tests\")\n",
    "# Red Model ALpha Config\n",
    "for a in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "    print(\"Alpha: \", a)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=108, alpha=a, max_iter=300)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_red_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_red_validation, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_red_validation), Y_validation), \"\\n\")\n",
    "    \n",
    "print(\"Green Config Alpha Tests\")\n",
    "# Green Model ALpha Config\n",
    "for a in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "    print(\"Alpha: \", a)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=108, alpha=a, max_iter=300)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_green_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_green_validation, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_green_validation), Y_validation), \"\\n\")\n",
    "    \n",
    "print(\"Blue Config Alpha Tests\")\n",
    "# Blue Model ALpha Config\n",
    "for a in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "    print(\"Alpha: \", a)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=108, alpha=a, max_iter=300)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_blue_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_blue_validation, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_blue_validation), Y_validation), \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd956067",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98        75\n",
      "           1       0.96      1.00      0.98        96\n",
      "           2       1.00      0.98      0.99        99\n",
      "           3       1.00      0.99      0.99        73\n",
      "           4       1.00      1.00      1.00        92\n",
      "           5       0.96      0.99      0.98       147\n",
      "\n",
      "   micro avg       0.98      0.99      0.99       582\n",
      "   macro avg       0.98      0.99      0.99       582\n",
      "weighted avg       0.98      0.99      0.99       582\n",
      " samples avg       0.98      0.99      0.98       582\n",
      "\n",
      "   Youdens Index\n",
      "0       0.971361\n",
      "1       0.991770\n",
      "2       0.979798\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       0.979404 \n",
      "\n",
      "Green Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        75\n",
      "           1       1.00      0.99      0.99        96\n",
      "           2       0.96      0.97      0.96        99\n",
      "           3       0.97      0.95      0.96        73\n",
      "           4       1.00      1.00      1.00        92\n",
      "           5       0.97      1.00      0.99       147\n",
      "\n",
      "   micro avg       0.98      0.99      0.98       582\n",
      "   macro avg       0.98      0.98      0.98       582\n",
      "weighted avg       0.98      0.99      0.98       582\n",
      " samples avg       0.98      0.99      0.98       582\n",
      "\n",
      "   Youdens Index\n",
      "0       0.998028\n",
      "1       0.989583\n",
      "2       0.961415\n",
      "3       0.941276\n",
      "4       1.000000\n",
      "5       0.990805 \n",
      "\n",
      "Blue Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97        75\n",
      "           1       0.98      0.96      0.97        96\n",
      "           2       0.96      0.97      0.96        99\n",
      "           3       0.92      0.97      0.95        73\n",
      "           4       1.00      0.99      0.99        92\n",
      "           5       1.00      1.00      1.00       147\n",
      "\n",
      "   micro avg       0.97      0.98      0.98       582\n",
      "   macro avg       0.97      0.98      0.97       582\n",
      "weighted avg       0.97      0.98      0.98       582\n",
      " samples avg       0.97      0.98      0.97       582\n",
      "\n",
      "   Youdens Index\n",
      "0       0.967416\n",
      "1       0.954218\n",
      "2       0.961415\n",
      "3       0.960815\n",
      "4       0.989130\n",
      "5       1.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Red\n",
    "mlpc_red =  MLPClassifier(hidden_layer_sizes=(108), activation='relu', solver='adam', random_state=1)\n",
    "mlpc_red.fit(X_red_train, Y_train)\n",
    "vector_red = mlpc_red.predict(X_red_train)\n",
    "\n",
    "# Green\n",
    "mlpc_green = MLPClassifier(hidden_layer_sizes=(108), activation='relu', solver='adam', random_state=1)\n",
    "mlpc_green.fit(X_green_train, Y_train)\n",
    "vector_green = mlpc_green.predict(X_green_train)\n",
    "\n",
    "# Blue\n",
    "mlpc_blue = MLPClassifier(hidden_layer_sizes=(108), activation='relu', solver='adam', random_state=1)\n",
    "mlpc_blue.fit(X_blue_train, Y_train)\n",
    "vector_blue = mlpc_blue.predict(X_blue_train)\n",
    "\n",
    "# Validation\n",
    "vector_red_val = mlpc_red.predict(X_red_validation)\n",
    "vector_green_val = mlpc_green.predict(X_green_validation)\n",
    "vector_blue_val = mlpc_blue.predict(X_blue_validation)\n",
    "\n",
    "\n",
    "print(\"Red Model Validation\")\n",
    "red_val_predict = mlpc_red.predict(X_red_validation)\n",
    "print(classification_report(Y_validation, red_val_predict, zero_division=0))\n",
    "print(get_youdens_index(red_val_predict, Y_validation), \"\\n\")\n",
    "\n",
    "print(\"Green Model Validation\")\n",
    "green_val_predict = mlpc_green.predict(X_green_validation)\n",
    "print(classification_report(Y_validation, green_val_predict, zero_division=0))\n",
    "print(get_youdens_index(green_val_predict, Y_validation), \"\\n\")\n",
    "\n",
    "print(\"Blue Model Validation\")\n",
    "blue_val_predict = mlpc_blue.predict(X_blue_validation)\n",
    "print(classification_report(Y_validation, blue_val_predict, zero_division=0))\n",
    "print(get_youdens_index(blue_val_predict, Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9d75f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combi Config HL Tests\n",
      "Hidden layers:  18\n",
      "Score:  0.9982817869415808 Time:  0.47547411918640137\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       1.000000\n",
      "2       1.000000\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Hidden layers:  14\n",
      "Score:  0.9965635738831615 Time:  0.5238513946533203\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       1.000000\n",
      "2       0.979798\n",
      "3       1.000000\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Hidden layers:  (14, 7)\n",
      "Score:  0.9621993127147767 Time:  0.710254430770874\n",
      "   Youdens Index\n",
      "0       0.946667\n",
      "1       0.927083\n",
      "2       0.967627\n",
      "3       0.931507\n",
      "4       0.978261\n",
      "5       1.000000 \n",
      "\n",
      "Combi Config Alpha Tests\n",
      "Alpha:  0.0001\n",
      "Score:  1.0 Time:  0.5675153732299805\n",
      "   Youdens Index\n",
      "0            1.0\n",
      "1            1.0\n",
      "2            1.0\n",
      "3            1.0\n",
      "4            1.0\n",
      "5            1.0 \n",
      "\n",
      "Alpha:  0.001\n",
      "Score:  0.9982817869415808 Time:  0.5725200176239014\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       1.000000\n",
      "2       1.000000\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Alpha:  0.01\n",
      "Score:  1.0 Time:  0.5344855785369873\n",
      "   Youdens Index\n",
      "0            1.0\n",
      "1            1.0\n",
      "2            1.0\n",
      "3            1.0\n",
      "4            1.0\n",
      "5            1.0 \n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9982817869415808 Time:  0.5244762897491455\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       1.000000\n",
      "2       1.000000\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n",
      "Alpha:  1\n",
      "Score:  0.993127147766323 Time:  1.0149223804473877\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       1.000000\n",
      "2       0.979798\n",
      "3       0.972603\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stack the rgb predictions to get combi model values \n",
    "X_combined_train = np.column_stack((vector_red, vector_green, vector_blue)) # shape (1745, 18) - 18 features\n",
    "X_combined_val = np.column_stack((vector_red_val, vector_green_val, vector_blue_val)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Combi Config HL Test\n",
    "print(\"Combi Config HL Tests\")\n",
    "for hl in [(18), (14), (14, 7)]:\n",
    "    print(\"Hidden layers: \", hl)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hl, max_iter=1000)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_combined_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_combined_val, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_combined_val), Y_validation), \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Combi Config Alpha Tests\")\n",
    "# Combi Model ALpha Config\n",
    "for a in [0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "    print(\"Alpha: \", a)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=108, alpha=a, max_iter=300)\n",
    "    t0 = time.time()\n",
    "    mlp.fit(X_combined_train, Y_train)\n",
    "    t1 = time.time()\n",
    "    print(\"Score: \", mlp.score(X_combined_val, Y_validation), \"Time: \", t1 - t0)\n",
    "    print(get_youdens_index(mlp.predict(X_combined_val), Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf5a31cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        75\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      0.99      0.99        99\n",
      "           3       1.00      0.99      0.99        73\n",
      "           4       1.00      1.00      1.00        92\n",
      "           5       1.00      1.00      1.00       147\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       582\n",
      "   macro avg       1.00      1.00      1.00       582\n",
      "weighted avg       1.00      1.00      1.00       582\n",
      " samples avg       1.00      1.00      1.00       582\n",
      "\n",
      "   Youdens Index\n",
      "0       1.000000\n",
      "1       1.000000\n",
      "2       0.989899\n",
      "3       0.986301\n",
      "4       1.000000\n",
      "5       1.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combi Train\n",
    "mlpc_combi =  MLPClassifier(hidden_layer_sizes=(18), activation='relu', solver='adam', random_state=1, max_iter=300)\n",
    "mlpc_combi.fit(X_combined_train, Y_train)\n",
    "\n",
    "# Validation\n",
    "print(\"Combined Model Validation\")\n",
    "combi_val = mlpc_combi.predict(X_combined_val)\n",
    "print(classification_report(Y_validation, combi_val, zero_division=0))\n",
    "print(get_youdens_index(combi_val, Y_validation), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ce00ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Finished \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_data_path = fr'../Datasets/full/Training'\n",
    "X_red_train_final, X_red_validation_final, X_red_test_final, X_green_train_final, X_green_validation_final, X_green_test_final, X_blue_train_final, X_blue_validation_final, X_blue_test_final, Y_train_final, Y_validation_final, Y_test_final = process_files(full_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ba841ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Model Validation Final\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.28      0.41       131\n",
      "           1       0.84      0.87      0.85       119\n",
      "           2       0.00      0.00      0.00       128\n",
      "           3       0.62      0.37      0.47       131\n",
      "           4       0.00      0.00      0.00       129\n",
      "           5       0.94      0.38      0.54       131\n",
      "           6       0.79      0.91      0.84       122\n",
      "           7       0.67      0.14      0.23       131\n",
      "           8       0.00      0.00      0.00       131\n",
      "           9       0.63      0.50      0.55       115\n",
      "          10       0.71      0.37      0.49       131\n",
      "          11       0.58      0.39      0.47       131\n",
      "          12       0.82      0.56      0.66       178\n",
      "          13       0.00      0.00      0.00       131\n",
      "          14       0.92      0.89      0.90       114\n",
      "          15       0.78      0.82      0.80       131\n",
      "          16       0.87      0.73      0.80       131\n",
      "          17       0.80      0.87      0.83       121\n",
      "          18       0.88      0.66      0.75       131\n",
      "          19       0.86      0.52      0.65       120\n",
      "          20       0.97      1.00      0.98       123\n",
      "          21       0.99      0.56      0.71       131\n",
      "          22       0.91      0.95      0.93       131\n",
      "          23       0.47      0.93      0.63       131\n",
      "          24       0.66      0.34      0.45       131\n",
      "          25       0.75      0.54      0.63       187\n",
      "          26       0.98      1.00      0.99       131\n",
      "          27       0.50      0.37      0.42       197\n",
      "          28       0.49      0.52      0.50       197\n",
      "          29       1.00      0.99      1.00       131\n",
      "          30       0.95      1.00      0.97       131\n",
      "          31       0.87      0.81      0.84       131\n",
      "          32       0.78      0.51      0.62       121\n",
      "          33       0.93      0.60      0.73       131\n",
      "          34       0.63      0.36      0.46       131\n",
      "          35       0.84      0.79      0.82       120\n",
      "          36       0.95      0.57      0.71       123\n",
      "          37       0.63      0.73      0.68       105\n",
      "          38       0.74      0.62      0.67       125\n",
      "          39       0.76      0.76      0.76       131\n",
      "          40       0.92      0.86      0.89       125\n",
      "          41       0.87      0.91      0.89       187\n",
      "          42       0.47      0.34      0.39        79\n",
      "          43       0.89      0.68      0.77       131\n",
      "          44       0.98      0.98      0.98       263\n",
      "          45       0.48      0.37      0.42       131\n",
      "          46       0.64      0.67      0.65       131\n",
      "          47       0.76      0.82      0.79       131\n",
      "          48       0.91      0.92      0.91       131\n",
      "          49       0.89      0.90      0.89       126\n",
      "          50       0.50      0.11      0.19       131\n",
      "          51       0.86      0.48      0.62       131\n",
      "          52       0.56      0.42      0.48       131\n",
      "          53       0.86      0.85      0.86       124\n",
      "          54       0.98      0.92      0.95       131\n",
      "          55       0.73      0.23      0.35       131\n",
      "          56       0.67      0.58      0.62       125\n",
      "          57       0.51      0.53      0.52       126\n",
      "          58       0.81      0.66      0.73       131\n",
      "          59       0.84      0.54      0.66       131\n",
      "          60       0.88      0.44      0.58       131\n",
      "          61       0.20      0.01      0.01       131\n",
      "          62       0.50      0.09      0.15       131\n",
      "          63       0.00      0.00      0.00       131\n",
      "          64       0.68      0.77      0.72       131\n",
      "          65       0.74      0.81      0.77       114\n",
      "          66       0.67      0.37      0.48        81\n",
      "          67       0.74      0.26      0.38       131\n",
      "          68       0.70      0.39      0.50       197\n",
      "          69       0.92      0.92      0.92       131\n",
      "          70       0.00      0.00      0.00       131\n",
      "          71       0.50      0.09      0.15       128\n",
      "          72       0.75      0.90      0.82       174\n",
      "          73       0.84      0.61      0.70       143\n",
      "          74       0.78      0.43      0.56       120\n",
      "          75       0.82      0.12      0.21       117\n",
      "          76       0.48      0.86      0.62       128\n",
      "          77       0.54      0.11      0.18       131\n",
      "          78       0.86      0.56      0.68       131\n",
      "          79       0.00      0.00      0.00       131\n",
      "          80       0.74      0.80      0.77       197\n",
      "          81       0.79      0.49      0.60       131\n",
      "          82       0.85      0.34      0.48       131\n",
      "          83       0.99      0.35      0.52       186\n",
      "          84       0.78      0.86      0.82       131\n",
      "          85       0.91      0.57      0.70       187\n",
      "          86       0.93      0.81      0.87        81\n",
      "          87       0.46      0.05      0.08       131\n",
      "          88       0.90      0.53      0.67       178\n",
      "          89       0.66      0.55      0.60       190\n",
      "          90       0.87      0.41      0.56       131\n",
      "          91       0.92      0.55      0.69       131\n",
      "          92       0.93      0.94      0.94       119\n",
      "          93       0.81      0.49      0.61       187\n",
      "          94       0.77      0.44      0.56       178\n",
      "          95       0.79      0.45      0.57       178\n",
      "          96       0.86      0.63      0.73       131\n",
      "          97       0.93      0.87      0.90       131\n",
      "          98       0.67      0.69      0.68       131\n",
      "          99       0.58      0.73      0.65       131\n",
      "         100       0.86      0.68      0.76       131\n",
      "         101       0.95      0.83      0.89       120\n",
      "         102       0.77      0.89      0.83       112\n",
      "         103       0.92      0.99      0.95       241\n",
      "         104       0.45      0.27      0.34       131\n",
      "         105       0.68      0.21      0.33       121\n",
      "         106       0.62      0.07      0.12       120\n",
      "         107       0.50      0.26      0.34       121\n",
      "         108       0.86      0.42      0.57       120\n",
      "         109       0.43      0.11      0.17       120\n",
      "         110       0.92      0.18      0.29       131\n",
      "         111       0.92      0.92      0.92       131\n",
      "         112       0.53      0.37      0.43       131\n",
      "         113       0.95      0.97      0.96       131\n",
      "         114       0.86      0.85      0.85       130\n",
      "         115       0.96      0.87      0.91       131\n",
      "         116       0.51      0.30      0.38       197\n",
      "         117       0.90      0.82      0.86       131\n",
      "         118       0.85      0.44      0.58       131\n",
      "         119       0.88      0.87      0.87       197\n",
      "         120       0.84      0.59      0.69       180\n",
      "         121       0.85      0.84      0.85       197\n",
      "         122       0.90      0.95      0.92       128\n",
      "         123       0.68      0.95      0.79       131\n",
      "         124       0.86      0.86      0.86       183\n",
      "         125       0.83      0.97      0.89        99\n",
      "         126       0.55      0.44      0.49       122\n",
      "         127       0.95      0.82      0.88       127\n",
      "         128       0.64      0.77      0.70       197\n",
      "         129       0.60      0.48      0.53       127\n",
      "\n",
      "   micro avg       0.78      0.58      0.66     17956\n",
      "   macro avg       0.72      0.57      0.61     17956\n",
      "weighted avg       0.73      0.58      0.62     17956\n",
      " samples avg       0.54      0.58      0.55     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.281770\n",
      "1         0.864481\n",
      "2         0.000000\n",
      "3         0.372363\n",
      "4         0.000000\n",
      "5         0.381511\n",
      "6         0.908154\n",
      "7         0.136900\n",
      "8         0.000000\n",
      "9         0.493746\n",
      "10        0.372924\n",
      "11        0.387237\n",
      "12        0.554999\n",
      "13        0.000000\n",
      "14        0.885460\n",
      "15        0.815111\n",
      "16        0.732039\n",
      "17        0.866311\n",
      "18        0.655815\n",
      "19        0.516106\n",
      "20        0.999776\n",
      "21        0.557196\n",
      "22        0.945892\n",
      "23        0.923668\n",
      "24        0.342221\n",
      "25        0.538250\n",
      "26        0.999888\n",
      "27        0.361372\n",
      "28        0.516761\n",
      "29        0.992366\n",
      "30        0.999607\n",
      "31        0.808263\n",
      "32        0.511387\n",
      "33        0.595083\n",
      "34        0.357208\n",
      "35        0.790657\n",
      "36        0.568881\n",
      "37        0.730812\n",
      "38        0.614486\n",
      "39        0.761620\n",
      "40        0.863439\n",
      "41        0.912975\n",
      "42        0.340038\n",
      "43        0.678772\n",
      "44        0.984565\n",
      "45        0.371016\n",
      "46        0.668951\n",
      "47        0.814943\n",
      "48        0.915357\n",
      "49        0.896040\n",
      "50        0.113662\n",
      "51        0.480355\n",
      "52        0.417435\n",
      "53        0.853885\n",
      "54        0.923496\n",
      "55        0.228391\n",
      "56        0.581981\n",
      "57        0.528100\n",
      "58        0.663000\n",
      "59        0.541199\n",
      "60        0.434666\n",
      "61        0.007409\n",
      "62        0.090930\n",
      "63       -0.000281\n",
      "64        0.768356\n",
      "65        0.805224\n",
      "66        0.369531\n",
      "67        0.258869\n",
      "68        0.383929\n",
      "69        0.915413\n",
      "70       -0.000056\n",
      "71        0.085320\n",
      "72        0.899431\n",
      "73        0.607437\n",
      "74        0.432492\n",
      "75        0.119490\n",
      "76        0.852700\n",
      "77        0.106197\n",
      "78        0.556579\n",
      "79        0.000000\n",
      "80        0.798933\n",
      "81        0.487596\n",
      "82        0.335429\n",
      "83        0.354782\n",
      "84        0.860800\n",
      "85        0.571630\n",
      "86        0.814535\n",
      "87        0.045409\n",
      "88        0.527527\n",
      "89        0.544329\n",
      "90        0.411765\n",
      "91        0.549282\n",
      "92        0.940728\n",
      "93        0.485449\n",
      "94        0.442526\n",
      "95        0.448257\n",
      "96        0.625225\n",
      "97        0.869780\n",
      "98        0.692188\n",
      "99        0.728953\n",
      "100       0.678604\n",
      "101       0.833053\n",
      "102       0.891176\n",
      "103       0.986366\n",
      "104       0.264819\n",
      "105       0.214203\n",
      "106       0.066386\n",
      "107       0.254460\n",
      "108       0.424551\n",
      "109       0.107380\n",
      "110       0.175460\n",
      "111       0.923047\n",
      "112       0.364000\n",
      "113       0.969073\n",
      "114       0.845144\n",
      "115       0.869949\n",
      "116       0.301303\n",
      "117       0.816121\n",
      "118       0.442187\n",
      "119       0.866725\n",
      "120       0.593263\n",
      "121       0.835987\n",
      "122       0.952340\n",
      "123       0.943311\n",
      "124       0.861981\n",
      "125       0.968577\n",
      "126       0.440156\n",
      "127       0.818617\n",
      "128       0.766787\n",
      "129       0.478015 \n",
      "\n",
      "Green Model Validation Final\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       131\n",
      "           1       0.97      0.97      0.97       119\n",
      "           2       0.87      0.10      0.18       128\n",
      "           3       0.88      0.69      0.77       131\n",
      "           4       0.82      0.88      0.85       129\n",
      "           5       0.78      1.00      0.88       131\n",
      "           6       0.91      0.84      0.88       122\n",
      "           7       0.93      0.76      0.83       131\n",
      "           8       0.75      0.91      0.82       131\n",
      "           9       0.96      0.69      0.80       115\n",
      "          10       0.96      0.90      0.93       131\n",
      "          11       0.92      0.91      0.92       131\n",
      "          12       0.93      0.90      0.91       178\n",
      "          13       0.82      0.81      0.82       131\n",
      "          14       0.92      0.93      0.93       114\n",
      "          15       0.93      0.68      0.78       131\n",
      "          16       0.93      0.81      0.87       131\n",
      "          17       0.88      0.92      0.90       121\n",
      "          18       0.92      0.87      0.89       131\n",
      "          19       0.81      0.56      0.66       120\n",
      "          20       0.96      0.97      0.96       123\n",
      "          21       0.95      0.68      0.79       131\n",
      "          22       0.97      0.53      0.68       131\n",
      "          23       0.75      0.94      0.84       131\n",
      "          24       0.80      0.86      0.83       131\n",
      "          25       0.95      0.71      0.81       187\n",
      "          26       1.00      0.98      0.99       131\n",
      "          27       0.95      0.92      0.94       197\n",
      "          28       0.85      0.81      0.83       197\n",
      "          29       0.99      1.00      1.00       131\n",
      "          30       1.00      0.98      0.99       131\n",
      "          31       0.97      0.99      0.98       131\n",
      "          32       0.94      0.72      0.81       121\n",
      "          33       1.00      0.79      0.88       131\n",
      "          34       0.78      0.65      0.71       131\n",
      "          35       0.94      0.81      0.87       120\n",
      "          36       1.00      0.69      0.82       123\n",
      "          37       0.97      0.80      0.88       105\n",
      "          38       0.90      0.82      0.86       125\n",
      "          39       0.94      0.82      0.88       131\n",
      "          40       0.93      0.91      0.92       125\n",
      "          41       0.98      0.87      0.92       187\n",
      "          42       0.67      0.71      0.69        79\n",
      "          43       0.97      0.68      0.80       131\n",
      "          44       1.00      0.99      0.99       263\n",
      "          45       0.94      0.75      0.83       131\n",
      "          46       0.89      0.77      0.83       131\n",
      "          47       0.83      0.54      0.65       131\n",
      "          48       0.93      0.93      0.93       131\n",
      "          49       0.97      0.82      0.89       126\n",
      "          50       0.82      0.42      0.56       131\n",
      "          51       0.72      0.89      0.80       131\n",
      "          52       0.88      0.69      0.78       131\n",
      "          53       0.95      0.82      0.88       124\n",
      "          54       0.95      0.96      0.95       131\n",
      "          55       0.70      0.68      0.69       131\n",
      "          56       0.64      0.54      0.59       125\n",
      "          57       0.89      0.88      0.88       126\n",
      "          58       0.94      0.57      0.71       131\n",
      "          59       0.57      0.40      0.47       131\n",
      "          60       0.83      0.65      0.73       131\n",
      "          61       0.80      0.46      0.58       131\n",
      "          62       0.72      0.66      0.69       131\n",
      "          63       0.66      0.50      0.57       131\n",
      "          64       0.87      0.83      0.85       131\n",
      "          65       0.93      0.98      0.96       114\n",
      "          66       0.73      0.78      0.75        81\n",
      "          67       0.83      0.89      0.86       131\n",
      "          68       0.89      0.59      0.71       197\n",
      "          69       0.93      0.95      0.94       131\n",
      "          70       0.87      0.93      0.90       131\n",
      "          71       0.94      0.91      0.92       128\n",
      "          72       0.94      0.94      0.94       174\n",
      "          73       0.89      0.57      0.70       143\n",
      "          74       0.71      0.78      0.74       120\n",
      "          75       0.87      0.64      0.74       117\n",
      "          76       0.97      0.84      0.90       128\n",
      "          77       0.73      0.71      0.72       131\n",
      "          78       0.87      0.69      0.77       131\n",
      "          79       0.89      0.82      0.85       131\n",
      "          80       0.96      0.88      0.92       197\n",
      "          81       0.80      0.86      0.83       131\n",
      "          82       0.68      0.63      0.65       131\n",
      "          83       0.81      0.85      0.83       186\n",
      "          84       0.99      0.92      0.95       131\n",
      "          85       0.87      0.92      0.90       187\n",
      "          86       0.97      0.83      0.89        81\n",
      "          87       0.87      0.61      0.72       131\n",
      "          88       0.88      0.89      0.88       178\n",
      "          89       0.83      0.85      0.84       190\n",
      "          90       0.90      0.58      0.71       131\n",
      "          91       0.94      0.63      0.76       131\n",
      "          92       0.98      0.92      0.95       119\n",
      "          93       0.86      0.74      0.80       187\n",
      "          94       0.99      1.00      0.99       178\n",
      "          95       0.74      0.85      0.79       178\n",
      "          96       0.90      0.95      0.93       131\n",
      "          97       0.86      0.97      0.91       131\n",
      "          98       0.95      0.60      0.74       131\n",
      "          99       0.89      0.89      0.89       131\n",
      "         100       0.92      0.93      0.93       131\n",
      "         101       0.94      0.97      0.96       120\n",
      "         102       1.00      1.00      1.00       112\n",
      "         103       0.93      0.95      0.94       241\n",
      "         104       0.90      0.85      0.87       131\n",
      "         105       0.80      0.60      0.68       121\n",
      "         106       0.50      0.52      0.51       120\n",
      "         107       0.89      0.76      0.82       121\n",
      "         108       0.71      0.68      0.70       120\n",
      "         109       0.75      0.41      0.53       120\n",
      "         110       0.87      0.76      0.81       131\n",
      "         111       0.93      0.83      0.88       131\n",
      "         112       0.90      0.80      0.85       131\n",
      "         113       0.97      0.97      0.97       131\n",
      "         114       0.94      0.92      0.93       130\n",
      "         115       0.98      0.95      0.97       131\n",
      "         116       0.92      0.77      0.83       197\n",
      "         117       1.00      1.00      1.00       131\n",
      "         118       0.83      0.69      0.75       131\n",
      "         119       0.98      1.00      0.99       197\n",
      "         120       0.99      0.89      0.94       180\n",
      "         121       1.00      0.89      0.94       197\n",
      "         122       0.99      1.00      1.00       128\n",
      "         123       0.99      0.98      0.98       131\n",
      "         124       1.00      1.00      1.00       183\n",
      "         125       0.97      0.96      0.96        99\n",
      "         126       0.79      0.21      0.34       122\n",
      "         127       0.68      0.85      0.76       127\n",
      "         128       0.71      0.78      0.74       197\n",
      "         129       0.88      0.52      0.65       127\n",
      "\n",
      "   micro avg       0.89      0.80      0.84     17956\n",
      "   macro avg       0.88      0.80      0.83     17956\n",
      "weighted avg       0.89      0.80      0.83     17956\n",
      " samples avg       0.77      0.80      0.78     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.991974\n",
      "1         0.974622\n",
      "2         0.101450\n",
      "3         0.693927\n",
      "4         0.882319\n",
      "5         0.997924\n",
      "6         0.843702\n",
      "7         0.755276\n",
      "8         0.906153\n",
      "9         0.686788\n",
      "10        0.900483\n",
      "11        0.907836\n",
      "12        0.903763\n",
      "13        0.807870\n",
      "14        0.929320\n",
      "15        0.678997\n",
      "16        0.808711\n",
      "17        0.916514\n",
      "18        0.869668\n",
      "19        0.557436\n",
      "20        0.967199\n",
      "21        0.679109\n",
      "22        0.526605\n",
      "23        0.936687\n",
      "24        0.860968\n",
      "25        0.705488\n",
      "26        0.984733\n",
      "27        0.923351\n",
      "28        0.810606\n",
      "29        0.999944\n",
      "30        0.984733\n",
      "31        0.992142\n",
      "32        0.718672\n",
      "33        0.786260\n",
      "34        0.647509\n",
      "35        0.807997\n",
      "36        0.691057\n",
      "37        0.799832\n",
      "38        0.823327\n",
      "39        0.824035\n",
      "40        0.911551\n",
      "41        0.866141\n",
      "42        0.707295\n",
      "43        0.679221\n",
      "44        0.992339\n",
      "45        0.747755\n",
      "46        0.770319\n",
      "47        0.541143\n",
      "48        0.930793\n",
      "49        0.817292\n",
      "50        0.419174\n",
      "51        0.883028\n",
      "52        0.693983\n",
      "53        0.822300\n",
      "54        0.961439\n",
      "55        0.677201\n",
      "56        0.533925\n",
      "57        0.880167\n",
      "58        0.572239\n",
      "59        0.402336\n",
      "60        0.647901\n",
      "61        0.457174\n",
      "62        0.654637\n",
      "63        0.494332\n",
      "64        0.831107\n",
      "65        0.982008\n",
      "66        0.776491\n",
      "67        0.884150\n",
      "68        0.588044\n",
      "69        0.953694\n",
      "70        0.930288\n",
      "71        0.905857\n",
      "72        0.936163\n",
      "73        0.572865\n",
      "74        0.772869\n",
      "75        0.640409\n",
      "76        0.843582\n",
      "77        0.707960\n",
      "78        0.686237\n",
      "79        0.816065\n",
      "80        0.877722\n",
      "81        0.860968\n",
      "82        0.623766\n",
      "83        0.852700\n",
      "84        0.915974\n",
      "85        0.918379\n",
      "86        0.827049\n",
      "87        0.610014\n",
      "88        0.886403\n",
      "89        0.850774\n",
      "90        0.579704\n",
      "91        0.633307\n",
      "92        0.924258\n",
      "93        0.742077\n",
      "94        0.999888\n",
      "95        0.850951\n",
      "96        0.953413\n",
      "97        0.968344\n",
      "98        0.602829\n",
      "99        0.892288\n",
      "100       0.930737\n",
      "101       0.974551\n",
      "102       1.000000\n",
      "103       0.945098\n",
      "104       0.846655\n",
      "105       0.594032\n",
      "106       0.513134\n",
      "107       0.759714\n",
      "108       0.681483\n",
      "109       0.407436\n",
      "110       0.762517\n",
      "111       0.831612\n",
      "112       0.800854\n",
      "113       0.969241\n",
      "114       0.914992\n",
      "115       0.954030\n",
      "116       0.765709\n",
      "117       1.000000\n",
      "118       0.685957\n",
      "119       0.999831\n",
      "120       0.894332\n",
      "121       0.893401\n",
      "122       0.999944\n",
      "123       0.977043\n",
      "124       1.000000\n",
      "125       0.959428\n",
      "126       0.212722\n",
      "127       0.847589\n",
      "128       0.773159\n",
      "129       0.519180 \n",
      "\n",
      "Blue Model Validation Final\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       131\n",
      "           1       0.99      1.00      1.00       119\n",
      "           2       0.86      0.88      0.87       128\n",
      "           3       0.91      0.85      0.88       131\n",
      "           4       0.90      1.00      0.95       129\n",
      "           5       0.98      0.98      0.98       131\n",
      "           6       0.94      0.93      0.94       122\n",
      "           7       0.86      0.95      0.91       131\n",
      "           8       0.95      0.97      0.96       131\n",
      "           9       0.98      0.89      0.93       115\n",
      "          10       0.91      0.69      0.79       131\n",
      "          11       0.91      0.89      0.90       131\n",
      "          12       1.00      0.94      0.97       178\n",
      "          13       0.98      1.00      0.99       131\n",
      "          14       0.98      1.00      0.99       114\n",
      "          15       0.91      0.79      0.84       131\n",
      "          16       0.96      0.92      0.94       131\n",
      "          17       0.97      0.92      0.94       121\n",
      "          18       0.96      0.89      0.92       131\n",
      "          19       0.62      0.81      0.70       120\n",
      "          20       0.92      1.00      0.96       123\n",
      "          21       0.97      0.81      0.88       131\n",
      "          22       0.60      0.98      0.75       131\n",
      "          23       0.82      0.97      0.89       131\n",
      "          24       0.93      0.93      0.93       131\n",
      "          25       0.95      0.96      0.96       187\n",
      "          26       1.00      0.98      0.99       131\n",
      "          27       0.92      0.97      0.95       197\n",
      "          28       0.79      0.81      0.80       197\n",
      "          29       1.00      0.98      0.99       131\n",
      "          30       1.00      1.00      1.00       131\n",
      "          31       0.98      0.99      0.98       131\n",
      "          32       0.98      0.85      0.91       121\n",
      "          33       0.95      0.98      0.97       131\n",
      "          34       0.93      0.53      0.68       131\n",
      "          35       0.99      0.81      0.89       120\n",
      "          36       0.87      0.92      0.89       123\n",
      "          37       0.92      0.82      0.87       105\n",
      "          38       1.00      0.99      1.00       125\n",
      "          39       0.97      0.82      0.89       131\n",
      "          40       0.98      0.94      0.96       125\n",
      "          41       0.78      0.91      0.84       187\n",
      "          42       0.88      0.53      0.66        79\n",
      "          43       0.75      0.89      0.82       131\n",
      "          44       0.97      0.94      0.95       263\n",
      "          45       0.94      0.89      0.91       131\n",
      "          46       0.62      0.96      0.76       131\n",
      "          47       0.78      1.00      0.88       131\n",
      "          48       0.95      0.95      0.95       131\n",
      "          49       0.95      0.97      0.96       126\n",
      "          50       0.99      0.99      0.99       131\n",
      "          51       1.00      0.96      0.98       131\n",
      "          52       0.95      0.97      0.96       131\n",
      "          53       0.96      0.89      0.92       124\n",
      "          54       0.88      0.93      0.90       131\n",
      "          55       0.77      0.92      0.84       131\n",
      "          56       0.87      0.83      0.85       125\n",
      "          57       0.92      0.57      0.71       126\n",
      "          58       0.86      0.92      0.89       131\n",
      "          59       0.79      0.94      0.86       131\n",
      "          60       1.00      0.98      0.99       131\n",
      "          61       0.94      0.72      0.81       131\n",
      "          62       0.85      0.78      0.81       131\n",
      "          63       0.92      0.96      0.94       131\n",
      "          64       0.98      0.94      0.96       131\n",
      "          65       0.96      0.96      0.96       114\n",
      "          66       0.91      0.75      0.82        81\n",
      "          67       0.96      0.94      0.95       131\n",
      "          68       0.90      0.91      0.91       197\n",
      "          69       1.00      0.92      0.96       131\n",
      "          70       0.90      0.99      0.95       131\n",
      "          71       0.98      0.96      0.97       128\n",
      "          72       0.97      0.95      0.96       174\n",
      "          73       0.84      0.79      0.82       143\n",
      "          74       0.88      0.55      0.68       120\n",
      "          75       0.75      0.96      0.84       117\n",
      "          76       0.98      0.99      0.99       128\n",
      "          77       0.98      0.93      0.95       131\n",
      "          78       0.74      0.92      0.82       131\n",
      "          79       0.97      0.88      0.92       131\n",
      "          80       0.97      0.95      0.96       197\n",
      "          81       0.98      0.92      0.95       131\n",
      "          82       0.98      0.81      0.89       131\n",
      "          83       0.95      0.85      0.90       186\n",
      "          84       0.98      0.92      0.95       131\n",
      "          85       0.96      0.95      0.95       187\n",
      "          86       0.93      0.86      0.90        81\n",
      "          87       0.89      0.79      0.84       131\n",
      "          88       0.82      0.92      0.87       178\n",
      "          89       0.93      0.77      0.84       190\n",
      "          90       0.95      0.79      0.86       131\n",
      "          91       0.91      0.90      0.90       131\n",
      "          92       0.94      0.90      0.92       119\n",
      "          93       0.96      0.94      0.95       187\n",
      "          94       0.99      0.98      0.99       178\n",
      "          95       0.97      0.98      0.98       178\n",
      "          96       0.96      0.98      0.97       131\n",
      "          97       0.89      0.89      0.89       131\n",
      "          98       0.89      0.80      0.84       131\n",
      "          99       0.96      0.82      0.88       131\n",
      "         100       0.76      0.93      0.84       131\n",
      "         101       0.96      0.94      0.95       120\n",
      "         102       1.00      1.00      1.00       112\n",
      "         103       0.99      0.91      0.95       241\n",
      "         104       0.68      0.95      0.79       131\n",
      "         105       0.83      0.99      0.91       121\n",
      "         106       0.73      0.61      0.66       120\n",
      "         107       0.83      0.87      0.85       121\n",
      "         108       0.94      0.54      0.69       120\n",
      "         109       0.82      0.63      0.71       120\n",
      "         110       0.91      0.86      0.89       131\n",
      "         111       0.96      0.89      0.92       131\n",
      "         112       0.98      0.92      0.95       131\n",
      "         113       0.98      0.98      0.98       131\n",
      "         114       0.96      0.97      0.97       130\n",
      "         115       0.98      0.92      0.95       131\n",
      "         116       0.95      0.80      0.87       197\n",
      "         117       0.98      0.99      0.99       131\n",
      "         118       0.99      0.99      0.99       131\n",
      "         119       0.99      1.00      1.00       197\n",
      "         120       0.94      0.91      0.92       180\n",
      "         121       0.98      0.93      0.96       197\n",
      "         122       1.00      1.00      1.00       128\n",
      "         123       0.98      0.99      0.98       131\n",
      "         124       0.99      1.00      1.00       183\n",
      "         125       0.99      1.00      0.99        99\n",
      "         126       0.99      0.98      0.99       122\n",
      "         127       0.98      0.99      0.98       127\n",
      "         128       0.94      0.99      0.97       197\n",
      "         129       0.92      0.88      0.90       127\n",
      "\n",
      "   micro avg       0.92      0.91      0.91     17956\n",
      "   macro avg       0.92      0.90      0.91     17956\n",
      "weighted avg       0.92      0.91      0.91     17956\n",
      " samples avg       0.88      0.91      0.89     17956\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Youdens Index\n",
      "0         0.976875\n",
      "1         0.999944\n",
      "2         0.881747\n",
      "3         0.846711\n",
      "4         0.999215\n",
      "5         0.976987\n",
      "6         0.934034\n",
      "7         0.953076\n",
      "8         0.969073\n",
      "9         0.886844\n",
      "10        0.694152\n",
      "11        0.892513\n",
      "12        0.943820\n",
      "13        0.999832\n",
      "14        0.999888\n",
      "15        0.785699\n",
      "16        0.915750\n",
      "17        0.917131\n",
      "18        0.885216\n",
      "19        0.805025\n",
      "20        0.999383\n",
      "21        0.808992\n",
      "22        0.979908\n",
      "23        0.967951\n",
      "24        0.930793\n",
      "25        0.962060\n",
      "26        0.977099\n",
      "27        0.968642\n",
      "28        0.809761\n",
      "29        0.984733\n",
      "30        1.000000\n",
      "31        0.992198\n",
      "32        0.851128\n",
      "33        0.984340\n",
      "34        0.534071\n",
      "35        0.808277\n",
      "36        0.917746\n",
      "37        0.818655\n",
      "38        0.992000\n",
      "39        0.816626\n",
      "40        0.935888\n",
      "41        0.906333\n",
      "42        0.531310\n",
      "43        0.890998\n",
      "44        0.934966\n",
      "45        0.885103\n",
      "46        0.957568\n",
      "47        0.997980\n",
      "48        0.953862\n",
      "49        0.967917\n",
      "50        0.992310\n",
      "51        0.961832\n",
      "52        0.969073\n",
      "53        0.886816\n",
      "54        0.930344\n",
      "55        0.921644\n",
      "56        0.831103\n",
      "57        0.571092\n",
      "58        0.922542\n",
      "59        0.937136\n",
      "60        0.977099\n",
      "61        0.717221\n",
      "62        0.777616\n",
      "63        0.961215\n",
      "64        0.938763\n",
      "65        0.964688\n",
      "66        0.752751\n",
      "67        0.938651\n",
      "68        0.912579\n",
      "69        0.923664\n",
      "70        0.991581\n",
      "71        0.960825\n",
      "72        0.947995\n",
      "73        0.789031\n",
      "74        0.549495\n",
      "75        0.955191\n",
      "76        0.992075\n",
      "77        0.931129\n",
      "78        0.913618\n",
      "79        0.877638\n",
      "80        0.954033\n",
      "81        0.915918\n",
      "82        0.809048\n",
      "83        0.854389\n",
      "84        0.923496\n",
      "85        0.951421\n",
      "86        0.863918\n",
      "87        0.793164\n",
      "88        0.919323\n",
      "89        0.767802\n",
      "90        0.793557\n",
      "91        0.900090\n",
      "92        0.898767\n",
      "93        0.935379\n",
      "94        0.983034\n",
      "95        0.982865\n",
      "96        0.984452\n",
      "97        0.892344\n",
      "98        0.800797\n",
      "99        0.816569\n",
      "100       0.929166\n",
      "101       0.941386\n",
      "102       1.000000\n",
      "103       0.912750\n",
      "104       0.950832\n",
      "105       0.990390\n",
      "106       0.606820\n",
      "107       0.866591\n",
      "108       0.541442\n",
      "109       0.632380\n",
      "110       0.861978\n",
      "111       0.885216\n",
      "112       0.923552\n",
      "113       0.984621\n",
      "114       0.968950\n",
      "115       0.923552\n",
      "116       0.796504\n",
      "117       0.992254\n",
      "118       0.992310\n",
      "119       0.999944\n",
      "120       0.904937\n",
      "121       0.933841\n",
      "122       1.000000\n",
      "123       0.992198\n",
      "124       0.999944\n",
      "125       0.999944\n",
      "126       0.983550\n",
      "127       0.991958\n",
      "128       0.994192\n",
      "129       0.881329 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Red Train Final\n",
    "mlpc_red_final =  MLPClassifier(hidden_layer_sizes=(108), activation='relu', solver='adam', random_state=1, max_iter=300)\n",
    "mlpc_red_final.fit(X_red_train_final, Y_train_final)\n",
    "vector_red_final = mlpc_red_final.predict(X_red_train_final)\n",
    "\n",
    "# Green Final\n",
    "mlpc_green_final = MLPClassifier(hidden_layer_sizes=(108), activation='relu', solver='adam', random_state=1, max_iter=300)\n",
    "mlpc_green_final.fit(X_green_train_final, Y_train_final)\n",
    "vector_green_final = mlpc_green_final.predict(X_green_train_final)\n",
    "\n",
    "# Blue Final\n",
    "mlpc_blue_final = MLPClassifier(hidden_layer_sizes=(108), activation='relu', solver='adam', random_state=1, max_iter=300)\n",
    "mlpc_blue_final.fit(X_blue_train_final, Y_train_final)\n",
    "vector_blue_final = mlpc_blue_final.predict(X_blue_train_final)\n",
    "\n",
    "# Validation Final\n",
    "vector_red_val_final = mlpc_red_final.predict(X_red_validation_final)\n",
    "vector_green_val_final = mlpc_green_final.predict(X_green_validation_final)\n",
    "vector_blue_val_final = mlpc_blue_final.predict(X_blue_validation_final)\n",
    "\n",
    "\n",
    "print(\"Red Model Validation Final\")\n",
    "red_val_predict_final = mlpc_red_final.predict(X_red_validation_final)\n",
    "print(classification_report(Y_validation_final, red_val_predict_final, zero_division=0))\n",
    "print(get_youdens_index(red_val_predict_final, Y_validation_final), \"\\n\")\n",
    "\n",
    "print(\"Green Model Validation Final\")\n",
    "green_val_predict_final = mlpc_green_final.predict(X_green_validation_final)\n",
    "print(classification_report(Y_validation_final, green_val_predict_final, zero_division=0))\n",
    "print(get_youdens_index(green_val_predict_final, Y_validation_final), \"\\n\")\n",
    "\n",
    "print(\"Blue Model Validation Final\")\n",
    "blue_val_predict_final = mlpc_blue_final.predict(X_blue_validation_final)\n",
    "print(classification_report(Y_validation_final, blue_val_predict_final, zero_division=0))\n",
    "print(get_youdens_index(blue_val_predict_final, Y_validation_final), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1987e850",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       131\n",
      "           1       1.00      0.99      1.00       119\n",
      "           2       0.93      0.88      0.91       128\n",
      "           3       0.97      0.89      0.93       131\n",
      "           4       0.98      0.97      0.98       129\n",
      "           5       0.98      0.96      0.97       131\n",
      "           6       1.00      0.97      0.98       122\n",
      "           7       0.97      0.96      0.97       131\n",
      "           8       0.99      0.97      0.98       131\n",
      "           9       1.00      0.92      0.96       115\n",
      "          10       1.00      0.93      0.96       131\n",
      "          11       0.98      0.97      0.98       131\n",
      "          12       0.99      0.97      0.98       178\n",
      "          13       1.00      1.00      1.00       131\n",
      "          14       0.99      0.97      0.98       114\n",
      "          15       0.98      0.91      0.94       131\n",
      "          16       0.99      0.90      0.94       131\n",
      "          17       0.99      0.92      0.95       121\n",
      "          18       0.99      0.91      0.95       131\n",
      "          19       0.91      0.78      0.84       120\n",
      "          20       1.00      1.00      1.00       123\n",
      "          21       0.99      0.83      0.90       131\n",
      "          22       0.93      1.00      0.96       131\n",
      "          23       0.98      0.97      0.98       131\n",
      "          24       0.96      0.91      0.93       131\n",
      "          25       0.97      0.96      0.97       187\n",
      "          26       1.00      1.00      1.00       131\n",
      "          27       0.97      0.98      0.98       197\n",
      "          28       0.94      0.96      0.95       197\n",
      "          29       1.00      0.99      1.00       131\n",
      "          30       1.00      1.00      1.00       131\n",
      "          31       0.99      0.98      0.99       131\n",
      "          32       0.97      0.83      0.89       121\n",
      "          33       0.99      0.98      0.98       131\n",
      "          34       0.92      0.76      0.83       131\n",
      "          35       0.98      0.82      0.90       120\n",
      "          36       0.96      0.91      0.93       123\n",
      "          37       0.99      0.70      0.82       105\n",
      "          38       1.00      0.97      0.98       125\n",
      "          39       0.95      0.89      0.92       131\n",
      "          40       1.00      0.95      0.98       125\n",
      "          41       1.00      0.93      0.96       187\n",
      "          42       0.89      0.81      0.85        79\n",
      "          43       1.00      0.91      0.95       131\n",
      "          44       1.00      0.99      1.00       263\n",
      "          45       0.97      0.89      0.93       131\n",
      "          46       0.92      0.95      0.94       131\n",
      "          47       0.96      0.98      0.97       131\n",
      "          48       0.99      0.99      0.99       131\n",
      "          49       0.99      0.98      0.98       126\n",
      "          50       0.98      0.99      0.99       131\n",
      "          51       1.00      0.97      0.98       131\n",
      "          52       0.98      0.98      0.98       131\n",
      "          53       0.99      0.94      0.97       124\n",
      "          54       1.00      0.98      0.99       131\n",
      "          55       0.97      0.92      0.95       131\n",
      "          56       0.91      0.86      0.88       125\n",
      "          57       0.93      0.89      0.91       126\n",
      "          58       0.97      0.90      0.93       131\n",
      "          59       0.96      0.95      0.96       131\n",
      "          60       1.00      0.97      0.98       131\n",
      "          61       0.95      0.82      0.88       131\n",
      "          62       0.95      0.84      0.89       131\n",
      "          63       0.98      0.96      0.97       131\n",
      "          64       0.98      0.95      0.97       131\n",
      "          65       1.00      0.98      0.99       114\n",
      "          66       0.95      0.75      0.84        81\n",
      "          67       0.98      0.97      0.97       131\n",
      "          68       0.96      0.90      0.93       197\n",
      "          69       0.99      0.98      0.99       131\n",
      "          70       0.98      0.99      0.98       131\n",
      "          71       0.98      0.98      0.98       128\n",
      "          72       0.99      0.96      0.98       174\n",
      "          73       0.91      0.80      0.85       143\n",
      "          74       0.87      0.81      0.84       120\n",
      "          75       0.93      0.88      0.90       117\n",
      "          76       0.98      0.99      0.99       128\n",
      "          77       0.98      0.98      0.98       131\n",
      "          78       0.95      0.92      0.93       131\n",
      "          79       0.98      0.95      0.96       131\n",
      "          80       1.00      0.99      0.99       197\n",
      "          81       0.96      0.96      0.96       131\n",
      "          82       0.96      0.83      0.89       131\n",
      "          83       0.96      0.90      0.93       186\n",
      "          84       0.99      0.94      0.96       131\n",
      "          85       1.00      0.94      0.97       187\n",
      "          86       0.99      0.83      0.90        81\n",
      "          87       0.96      0.80      0.87       131\n",
      "          88       0.97      0.96      0.96       178\n",
      "          89       0.96      0.91      0.93       190\n",
      "          90       0.93      0.85      0.89       131\n",
      "          91       0.97      0.89      0.93       131\n",
      "          92       0.98      0.97      0.98       119\n",
      "          93       0.99      0.93      0.96       187\n",
      "          94       1.00      1.00      1.00       178\n",
      "          95       0.99      0.98      0.99       178\n",
      "          96       0.98      0.97      0.98       131\n",
      "          97       0.98      0.97      0.97       131\n",
      "          98       0.95      0.81      0.87       131\n",
      "          99       0.98      0.97      0.97       131\n",
      "         100       0.98      0.94      0.96       131\n",
      "         101       0.99      0.95      0.97       120\n",
      "         102       1.00      0.99      1.00       112\n",
      "         103       1.00      1.00      1.00       241\n",
      "         104       0.95      0.95      0.95       131\n",
      "         105       0.98      0.98      0.98       121\n",
      "         106       0.87      0.64      0.74       120\n",
      "         107       0.96      0.87      0.91       121\n",
      "         108       0.89      0.69      0.78       120\n",
      "         109       0.88      0.63      0.74       120\n",
      "         110       0.98      0.89      0.93       131\n",
      "         111       0.98      0.97      0.97       131\n",
      "         112       0.99      0.95      0.97       131\n",
      "         113       1.00      0.99      1.00       131\n",
      "         114       1.00      0.96      0.98       130\n",
      "         115       1.00      0.95      0.97       131\n",
      "         116       0.96      0.87      0.91       197\n",
      "         117       1.00      0.99      1.00       131\n",
      "         118       1.00      0.98      0.99       131\n",
      "         119       1.00      1.00      1.00       197\n",
      "         120       0.99      0.94      0.96       180\n",
      "         121       1.00      0.96      0.98       197\n",
      "         122       1.00      1.00      1.00       128\n",
      "         123       1.00      0.98      0.99       131\n",
      "         124       1.00      1.00      1.00       183\n",
      "         125       1.00      0.99      0.99        99\n",
      "         126       1.00      0.98      0.99       122\n",
      "         127       0.98      1.00      0.99       127\n",
      "         128       0.96      0.98      0.97       197\n",
      "         129       0.98      0.91      0.94       127\n",
      "\n",
      "   micro avg       0.98      0.93      0.95     17956\n",
      "   macro avg       0.97      0.93      0.95     17956\n",
      "weighted avg       0.98      0.93      0.95     17956\n",
      " samples avg       0.93      0.93      0.93     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.984677\n",
      "1         0.991597\n",
      "2         0.882364\n",
      "3         0.892905\n",
      "4         0.968880\n",
      "5         0.961720\n",
      "6         0.967213\n",
      "7         0.961608\n",
      "8         0.969410\n",
      "9         0.921739\n",
      "10        0.931298\n",
      "11        0.969353\n",
      "12        0.966236\n",
      "13        1.000000\n",
      "14        0.973628\n",
      "15        0.908229\n",
      "16        0.900707\n",
      "17        0.917299\n",
      "18        0.908341\n",
      "19        0.774495\n",
      "20        1.000000\n",
      "21        0.832005\n",
      "22        0.999439\n",
      "23        0.969353\n",
      "24        0.908116\n",
      "25        0.962229\n",
      "26        1.000000\n",
      "27        0.984490\n",
      "28        0.958715\n",
      "29        0.992366\n",
      "30        1.000000\n",
      "31        0.984677\n",
      "32        0.826278\n",
      "33        0.977043\n",
      "34        0.762854\n",
      "35        0.824888\n",
      "36        0.910289\n",
      "37        0.704706\n",
      "38        0.968000\n",
      "39        0.892793\n",
      "40        0.952000\n",
      "41        0.925134\n",
      "42        0.809679\n",
      "43        0.908397\n",
      "44        0.992395\n",
      "45        0.885328\n",
      "46        0.953581\n",
      "47        0.976819\n",
      "48        0.992310\n",
      "49        0.976134\n",
      "50        0.992254\n",
      "51        0.969466\n",
      "52        0.976987\n",
      "53        0.943492\n",
      "54        0.977099\n",
      "55        0.923440\n",
      "56        0.855439\n",
      "57        0.888440\n",
      "58        0.900539\n",
      "59        0.953918\n",
      "60        0.969466\n",
      "61        0.816457\n",
      "62        0.839358\n",
      "63        0.961664\n",
      "64        0.954086\n",
      "65        0.982456\n",
      "66        0.752919\n",
      "67        0.969297\n",
      "68        0.903159\n",
      "69        0.984677\n",
      "70        0.992198\n",
      "71        0.984263\n",
      "72        0.959714\n",
      "73        0.796585\n",
      "74        0.807492\n",
      "75        0.879893\n",
      "76        0.992075\n",
      "77        0.976931\n",
      "78        0.923271\n",
      "79        0.946453\n",
      "80        0.989848\n",
      "81        0.961552\n",
      "82        0.831781\n",
      "83        0.902832\n",
      "84        0.938875\n",
      "85        0.941176\n",
      "86        0.827105\n",
      "87        0.801302\n",
      "88        0.954719\n",
      "89        0.904869\n",
      "90        0.854457\n",
      "91        0.892905\n",
      "92        0.974678\n",
      "93        0.925077\n",
      "94        1.000000\n",
      "95        0.977472\n",
      "96        0.969353\n",
      "97        0.969297\n",
      "98        0.808824\n",
      "99        0.969297\n",
      "100       0.938819\n",
      "101       0.949944\n",
      "102       0.991071\n",
      "103       0.999944\n",
      "104       0.953862\n",
      "105       0.975038\n",
      "106       0.640994\n",
      "107       0.867544\n",
      "108       0.691106\n",
      "109       0.632773\n",
      "110       0.885384\n",
      "111       0.969297\n",
      "112       0.954142\n",
      "113       0.992366\n",
      "114       0.961538\n",
      "115       0.946565\n",
      "116       0.867570\n",
      "117       0.992366\n",
      "118       0.984733\n",
      "119       1.000000\n",
      "120       0.938776\n",
      "121       0.959391\n",
      "122       1.000000\n",
      "123       0.984733\n",
      "124       1.000000\n",
      "125       0.989899\n",
      "126       0.983607\n",
      "127       0.999832\n",
      "128       0.984321\n",
      "129       0.905400 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stack the final rgb predictions to get final combi model values \n",
    "X_combined_train_final = np.column_stack((vector_red_final, vector_green_final, vector_blue_final)) # shape (1745, 18) - 18 features\n",
    "X_combined_val_final = np.column_stack((vector_red_val_final, vector_green_val_final, vector_blue_val_final)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Combi Train Final\n",
    "mlpc_combi_final =  MLPClassifier(hidden_layer_sizes=(18), activation='relu', solver='adam', random_state=1, max_iter=300)\n",
    "mlpc_combi_final.fit(X_combined_train_final, Y_train_final)\n",
    "\n",
    "# Validation Final\n",
    "print(\"Combined Model Validation\")\n",
    "combi_val_final = mlpc_combi_final.predict(X_combined_val_final)\n",
    "print(classification_report(Y_validation_final, combi_val_final, zero_division=0))\n",
    "print(get_youdens_index(combi_val_final, Y_validation_final), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60fbc463",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Model Test Final\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.27      0.39       131\n",
      "           1       0.83      0.90      0.86       118\n",
      "           2       0.00      0.00      0.00       128\n",
      "           3       0.65      0.47      0.55       131\n",
      "           4       0.00      0.00      0.00       128\n",
      "           5       0.92      0.35      0.51       132\n",
      "           6       0.79      0.91      0.85       122\n",
      "           7       0.66      0.15      0.24       131\n",
      "           8       0.00      0.00      0.00       131\n",
      "           9       0.54      0.50      0.52       115\n",
      "          10       0.73      0.40      0.51       131\n",
      "          11       0.55      0.40      0.46       131\n",
      "          12       0.79      0.60      0.69       179\n",
      "          13       0.00      0.00      0.00       131\n",
      "          14       0.89      0.78      0.83       114\n",
      "          15       0.79      0.79      0.79       132\n",
      "          16       0.80      0.66      0.72       131\n",
      "          17       0.75      0.70      0.72       121\n",
      "          18       0.95      0.70      0.81       131\n",
      "          19       0.84      0.51      0.63       120\n",
      "          20       0.97      0.98      0.97       123\n",
      "          21       0.94      0.55      0.69       131\n",
      "          22       0.95      0.90      0.93       131\n",
      "          23       0.46      0.93      0.61       131\n",
      "          24       0.64      0.21      0.32       131\n",
      "          25       0.80      0.63      0.71       187\n",
      "          26       0.99      0.98      0.99       131\n",
      "          27       0.57      0.46      0.51       197\n",
      "          28       0.59      0.49      0.54       197\n",
      "          29       0.99      0.99      0.99       131\n",
      "          30       0.87      1.00      0.93       131\n",
      "          31       0.83      0.79      0.81       132\n",
      "          32       0.72      0.45      0.56       121\n",
      "          33       0.90      0.58      0.71       131\n",
      "          34       0.60      0.41      0.49       131\n",
      "          35       0.89      0.79      0.84       120\n",
      "          36       0.97      0.57      0.72       123\n",
      "          37       0.71      0.79      0.75       104\n",
      "          38       0.74      0.60      0.66       125\n",
      "          39       0.75      0.77      0.76       131\n",
      "          40       0.96      0.90      0.93       125\n",
      "          41       0.83      0.91      0.87       187\n",
      "          42       0.45      0.37      0.41        79\n",
      "          43       0.93      0.80      0.86       131\n",
      "          44       0.99      1.00      0.99       262\n",
      "          45       0.53      0.48      0.50       131\n",
      "          46       0.66      0.74      0.70       131\n",
      "          47       0.77      0.80      0.78       131\n",
      "          48       0.93      0.85      0.88       131\n",
      "          49       0.86      0.86      0.86       126\n",
      "          50       0.47      0.14      0.21       131\n",
      "          51       0.74      0.38      0.50       131\n",
      "          52       0.52      0.37      0.44       131\n",
      "          53       0.82      0.83      0.83       124\n",
      "          54       0.99      0.94      0.96       131\n",
      "          55       0.78      0.24      0.37       131\n",
      "          56       0.62      0.65      0.63       124\n",
      "          57       0.55      0.53      0.54       126\n",
      "          58       0.87      0.57      0.69       131\n",
      "          59       0.80      0.47      0.59       131\n",
      "          60       0.82      0.46      0.59       131\n",
      "          61       0.00      0.00      0.00       131\n",
      "          62       0.65      0.13      0.22       131\n",
      "          63       0.00      0.00      0.00       131\n",
      "          64       0.64      0.74      0.69       131\n",
      "          65       0.75      0.68      0.71       114\n",
      "          66       0.70      0.44      0.54        80\n",
      "          67       0.64      0.18      0.28       131\n",
      "          68       0.70      0.40      0.51       197\n",
      "          69       0.90      0.92      0.91       131\n",
      "          70       0.33      0.02      0.03       131\n",
      "          71       0.58      0.09      0.15       128\n",
      "          72       0.77      0.90      0.83       175\n",
      "          73       0.76      0.58      0.66       142\n",
      "          74       0.84      0.38      0.53       120\n",
      "          75       0.79      0.09      0.17       117\n",
      "          76       0.50      0.84      0.63       128\n",
      "          77       0.32      0.09      0.14       131\n",
      "          78       0.84      0.50      0.63       131\n",
      "          79       0.00      0.00      0.00       131\n",
      "          80       0.77      0.75      0.76       197\n",
      "          81       0.82      0.47      0.60       131\n",
      "          82       0.84      0.37      0.52       131\n",
      "          83       0.95      0.38      0.54       186\n",
      "          84       0.90      0.85      0.88       131\n",
      "          85       0.89      0.62      0.73       188\n",
      "          86       0.94      0.78      0.85        80\n",
      "          87       0.50      0.02      0.03       131\n",
      "          88       0.93      0.59      0.72       178\n",
      "          89       0.74      0.56      0.64       190\n",
      "          90       0.83      0.51      0.63       132\n",
      "          91       0.97      0.52      0.68       132\n",
      "          92       0.93      0.92      0.93       118\n",
      "          93       0.84      0.53      0.65       187\n",
      "          94       0.77      0.46      0.58       178\n",
      "          95       0.76      0.47      0.58       178\n",
      "          96       0.83      0.67      0.74       131\n",
      "          97       0.93      0.79      0.86       131\n",
      "          98       0.71      0.76      0.73       131\n",
      "          99       0.58      0.70      0.64       132\n",
      "         100       0.82      0.60      0.70       131\n",
      "         101       0.91      0.88      0.90       120\n",
      "         102       0.78      0.92      0.85       113\n",
      "         103       0.92      0.99      0.95       241\n",
      "         104       0.60      0.33      0.42       131\n",
      "         105       0.70      0.26      0.38       121\n",
      "         106       0.67      0.08      0.15       120\n",
      "         107       0.55      0.26      0.36       121\n",
      "         108       0.80      0.31      0.45       120\n",
      "         109       0.57      0.17      0.27       120\n",
      "         110       0.92      0.18      0.29       131\n",
      "         111       0.92      0.88      0.90       131\n",
      "         112       0.55      0.31      0.40       131\n",
      "         113       0.98      0.98      0.98       131\n",
      "         114       0.85      0.86      0.86       131\n",
      "         115       0.97      0.86      0.91       131\n",
      "         116       0.55      0.30      0.39       197\n",
      "         117       0.86      0.79      0.82       131\n",
      "         118       0.89      0.41      0.56       131\n",
      "         119       0.88      0.86      0.87       197\n",
      "         120       0.82      0.59      0.69       179\n",
      "         121       0.79      0.87      0.83       197\n",
      "         122       0.89      0.95      0.92       128\n",
      "         123       0.65      0.91      0.76       131\n",
      "         124       0.83      0.83      0.83       183\n",
      "         125       0.82      0.93      0.87        99\n",
      "         126       0.60      0.46      0.52       123\n",
      "         127       0.92      0.86      0.89       126\n",
      "         128       0.66      0.75      0.70       197\n",
      "         129       0.68      0.55      0.61       126\n",
      "\n",
      "   micro avg       0.78      0.58      0.66     17956\n",
      "   macro avg       0.73      0.57      0.61     17956\n",
      "weighted avg       0.73      0.58      0.62     17956\n",
      " samples avg       0.54      0.58      0.55     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.273687\n",
      "1         0.897072\n",
      "2         0.000000\n",
      "3         0.471375\n",
      "4        -0.000168\n",
      "5         0.348260\n",
      "6         0.908210\n",
      "7         0.144477\n",
      "8         0.000000\n",
      "9         0.501545\n",
      "10        0.395881\n",
      "11        0.394590\n",
      "12        0.601777\n",
      "13        0.000000\n",
      "14        0.780085\n",
      "15        0.786308\n",
      "16        0.662888\n",
      "17        0.700853\n",
      "18        0.702010\n",
      "19        0.507661\n",
      "20        0.975385\n",
      "21        0.549338\n",
      "22        0.900427\n",
      "23        0.923163\n",
      "24        0.212843\n",
      "25        0.629384\n",
      "26        0.984677\n",
      "27        0.452967\n",
      "28        0.488557\n",
      "29        0.992310\n",
      "30        0.998934\n",
      "31        0.786644\n",
      "32        0.453368\n",
      "33        0.579704\n",
      "34        0.410194\n",
      "35        0.790994\n",
      "36        0.568994\n",
      "37        0.786557\n",
      "38        0.598542\n",
      "39        0.769141\n",
      "40        0.895720\n",
      "41        0.912525\n",
      "42        0.365131\n",
      "43        0.801078\n",
      "44        0.996070\n",
      "45        0.477718\n",
      "46        0.737597\n",
      "47        0.799731\n",
      "48        0.846823\n",
      "49        0.856133\n",
      "50        0.136283\n",
      "51        0.380670\n",
      "52        0.371521\n",
      "53        0.829411\n",
      "54        0.938875\n",
      "55        0.243770\n",
      "56        0.642470\n",
      "57        0.528717\n",
      "58        0.571902\n",
      "59        0.464807\n",
      "60        0.457286\n",
      "61       -0.000449\n",
      "62        0.129266\n",
      "63       -0.000168\n",
      "64        0.737372\n",
      "65        0.674037\n",
      "66        0.436661\n",
      "67        0.174843\n",
      "68        0.399101\n",
      "69        0.915301\n",
      "70        0.015043\n",
      "71        0.085489\n",
      "72        0.900214\n",
      "73        0.583048\n",
      "74        0.382829\n",
      "75        0.093849\n",
      "76        0.829992\n",
      "77        0.090201\n",
      "78        0.503087\n",
      "79        0.000000\n",
      "80        0.748848\n",
      "81        0.472497\n",
      "82        0.373541\n",
      "83        0.376119\n",
      "84        0.854233\n",
      "85        0.621496\n",
      "86        0.774776\n",
      "87        0.015155\n",
      "88        0.589438\n",
      "89        0.555812\n",
      "90        0.506790\n",
      "91        0.522615\n",
      "92        0.923280\n",
      "93        0.528342\n",
      "94        0.459324\n",
      "95        0.470448\n",
      "96        0.670746\n",
      "97        0.793444\n",
      "98        0.753481\n",
      "99        0.700786\n",
      "100       0.602100\n",
      "101       0.882773\n",
      "102       0.918729\n",
      "103       0.986366\n",
      "104       0.326617\n",
      "105       0.255469\n",
      "106       0.083053\n",
      "107       0.263005\n",
      "108       0.307829\n",
      "109       0.174103\n",
      "110       0.175460\n",
      "111       0.877302\n",
      "112       0.311126\n",
      "113       0.984565\n",
      "114       0.861473\n",
      "115       0.862427\n",
      "116       0.301753\n",
      "117       0.785306\n",
      "118       0.411821\n",
      "119       0.856517\n",
      "120       0.590885\n",
      "121       0.870506\n",
      "122       0.944471\n",
      "123       0.904806\n",
      "124       0.823449\n",
      "125       0.928173\n",
      "126       0.461284\n",
      "127       0.856582\n",
      "128       0.741913\n",
      "129       0.545768 \n",
      "\n",
      "Green Model Test Final\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       131\n",
      "           1       1.00      0.97      0.99       118\n",
      "           2       0.77      0.08      0.14       128\n",
      "           3       0.84      0.83      0.84       131\n",
      "           4       0.79      0.86      0.82       128\n",
      "           5       0.78      0.98      0.87       132\n",
      "           6       0.93      0.93      0.93       122\n",
      "           7       0.94      0.78      0.85       131\n",
      "           8       0.70      0.96      0.81       131\n",
      "           9       0.95      0.70      0.80       115\n",
      "          10       0.97      0.91      0.94       131\n",
      "          11       0.90      0.90      0.90       131\n",
      "          12       0.93      0.92      0.92       179\n",
      "          13       0.77      0.78      0.77       131\n",
      "          14       0.90      0.86      0.88       114\n",
      "          15       0.95      0.63      0.76       132\n",
      "          16       0.90      0.87      0.89       131\n",
      "          17       0.94      0.84      0.89       121\n",
      "          18       0.96      0.89      0.92       131\n",
      "          19       0.76      0.54      0.63       120\n",
      "          20       0.96      0.96      0.96       123\n",
      "          21       0.90      0.72      0.80       131\n",
      "          22       0.98      0.43      0.60       131\n",
      "          23       0.73      0.92      0.81       131\n",
      "          24       0.80      0.79      0.80       131\n",
      "          25       0.95      0.70      0.80       187\n",
      "          26       0.99      0.99      0.99       131\n",
      "          27       0.96      0.94      0.95       197\n",
      "          28       0.79      0.75      0.77       197\n",
      "          29       0.99      0.99      0.99       131\n",
      "          30       1.00      0.99      1.00       131\n",
      "          31       0.97      1.00      0.99       132\n",
      "          32       0.88      0.76      0.81       121\n",
      "          33       0.95      0.78      0.86       131\n",
      "          34       0.76      0.62      0.68       131\n",
      "          35       0.89      0.78      0.83       120\n",
      "          36       1.00      0.70      0.82       123\n",
      "          37       0.96      0.88      0.91       104\n",
      "          38       0.93      0.90      0.91       125\n",
      "          39       0.90      0.85      0.88       131\n",
      "          40       0.99      0.92      0.95       125\n",
      "          41       0.98      0.85      0.91       187\n",
      "          42       0.73      0.70      0.71        79\n",
      "          43       0.94      0.76      0.84       131\n",
      "          44       0.98      0.98      0.98       262\n",
      "          45       0.96      0.77      0.86       131\n",
      "          46       0.92      0.82      0.87       131\n",
      "          47       0.95      0.66      0.78       131\n",
      "          48       0.92      0.94      0.93       131\n",
      "          49       0.95      0.82      0.88       126\n",
      "          50       0.89      0.44      0.58       131\n",
      "          51       0.77      0.88      0.82       131\n",
      "          52       0.89      0.63      0.74       131\n",
      "          53       0.91      0.84      0.87       124\n",
      "          54       0.94      0.95      0.95       131\n",
      "          55       0.78      0.63      0.70       131\n",
      "          56       0.70      0.64      0.67       124\n",
      "          57       0.85      0.89      0.87       126\n",
      "          58       0.93      0.52      0.67       131\n",
      "          59       0.52      0.34      0.41       131\n",
      "          60       0.86      0.64      0.73       131\n",
      "          61       0.71      0.30      0.42       131\n",
      "          62       0.78      0.64      0.70       131\n",
      "          63       0.65      0.57      0.61       131\n",
      "          64       0.89      0.91      0.90       131\n",
      "          65       0.93      0.96      0.95       114\n",
      "          66       0.80      0.76      0.78        80\n",
      "          67       0.85      0.90      0.87       131\n",
      "          68       0.87      0.60      0.71       197\n",
      "          69       0.94      0.95      0.95       131\n",
      "          70       0.93      0.97      0.95       131\n",
      "          71       0.95      0.91      0.93       128\n",
      "          72       0.90      0.96      0.93       175\n",
      "          73       0.90      0.58      0.70       142\n",
      "          74       0.65      0.68      0.67       120\n",
      "          75       0.87      0.56      0.68       117\n",
      "          76       0.99      0.78      0.87       128\n",
      "          77       0.79      0.76      0.77       131\n",
      "          78       0.88      0.75      0.81       131\n",
      "          79       0.77      0.74      0.75       131\n",
      "          80       0.94      0.88      0.91       197\n",
      "          81       0.78      0.87      0.82       131\n",
      "          82       0.70      0.67      0.68       131\n",
      "          83       0.84      0.89      0.86       186\n",
      "          84       1.00      0.90      0.95       131\n",
      "          85       0.88      0.91      0.90       188\n",
      "          86       0.95      0.89      0.92        80\n",
      "          87       0.81      0.66      0.73       131\n",
      "          88       0.86      0.90      0.88       178\n",
      "          89       0.81      0.87      0.84       190\n",
      "          90       0.95      0.68      0.79       132\n",
      "          91       0.94      0.64      0.77       132\n",
      "          92       0.97      0.92      0.94       118\n",
      "          93       0.88      0.80      0.84       187\n",
      "          94       0.98      0.99      0.99       178\n",
      "          95       0.71      0.84      0.77       178\n",
      "          96       0.91      0.98      0.95       131\n",
      "          97       0.89      0.97      0.93       131\n",
      "          98       0.91      0.61      0.73       131\n",
      "          99       0.85      0.89      0.87       132\n",
      "         100       0.85      0.81      0.83       131\n",
      "         101       0.98      0.99      0.99       120\n",
      "         102       0.99      1.00      1.00       113\n",
      "         103       0.92      0.95      0.93       241\n",
      "         104       0.91      0.85      0.88       131\n",
      "         105       0.88      0.58      0.70       121\n",
      "         106       0.46      0.44      0.45       120\n",
      "         107       0.90      0.76      0.83       121\n",
      "         108       0.78      0.65      0.71       120\n",
      "         109       0.70      0.37      0.48       120\n",
      "         110       0.85      0.79      0.82       131\n",
      "         111       0.92      0.75      0.83       131\n",
      "         112       0.94      0.85      0.90       131\n",
      "         113       0.99      0.99      0.99       131\n",
      "         114       0.94      0.92      0.93       131\n",
      "         115       0.98      0.93      0.96       131\n",
      "         116       0.91      0.71      0.79       197\n",
      "         117       1.00      0.97      0.98       131\n",
      "         118       0.89      0.75      0.81       131\n",
      "         119       0.98      1.00      0.99       197\n",
      "         120       0.95      0.84      0.89       179\n",
      "         121       0.99      0.87      0.93       197\n",
      "         122       1.00      1.00      1.00       128\n",
      "         123       1.00      0.98      0.99       131\n",
      "         124       1.00      1.00      1.00       183\n",
      "         125       0.96      0.94      0.95        99\n",
      "         126       0.88      0.36      0.51       123\n",
      "         127       0.75      0.93      0.83       126\n",
      "         128       0.74      0.71      0.73       197\n",
      "         129       0.89      0.54      0.67       126\n",
      "\n",
      "   micro avg       0.89      0.80      0.84     17956\n",
      "   macro avg       0.88      0.80      0.83     17956\n",
      "weighted avg       0.89      0.80      0.83     17956\n",
      " samples avg       0.77      0.80      0.78     17956\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Youdens Index\n",
      "0         0.999495\n",
      "1         0.974576\n",
      "2         0.077957\n",
      "3         0.830883\n",
      "4         0.857748\n",
      "5         0.982773\n",
      "6         0.933978\n",
      "7         0.778289\n",
      "8         0.958859\n",
      "9         0.695428\n",
      "10        0.908173\n",
      "11        0.900034\n",
      "12        0.915526\n",
      "13        0.776887\n",
      "14        0.859033\n",
      "15        0.628563\n",
      "16        0.869556\n",
      "17        0.842639\n",
      "18        0.885216\n",
      "19        0.540489\n",
      "20        0.959069\n",
      "21        0.716940\n",
      "22        0.427425\n",
      "23        0.913562\n",
      "24        0.792435\n",
      "25        0.694793\n",
      "26        0.992310\n",
      "27        0.943768\n",
      "28        0.744053\n",
      "29        0.992310\n",
      "30        0.992366\n",
      "31        0.999776\n",
      "32        0.759602\n",
      "33        0.778345\n",
      "34        0.616862\n",
      "35        0.782661\n",
      "36        0.699187\n",
      "37        0.874776\n",
      "38        0.895551\n",
      "39        0.854289\n",
      "40        0.919944\n",
      "41        0.850099\n",
      "42        0.695084\n",
      "43        0.763022\n",
      "44        0.980577\n",
      "45        0.770768\n",
      "46        0.823866\n",
      "47        0.663842\n",
      "48        0.938370\n",
      "49        0.817180\n",
      "50        0.434722\n",
      "51        0.875899\n",
      "52        0.633027\n",
      "53        0.838149\n",
      "54        0.953750\n",
      "55        0.632297\n",
      "56        0.635190\n",
      "57        0.887823\n",
      "58        0.518803\n",
      "59        0.333578\n",
      "60        0.640436\n",
      "61        0.296812\n",
      "62        0.639875\n",
      "63        0.570275\n",
      "64        0.907555\n",
      "65        0.964464\n",
      "66        0.761661\n",
      "67        0.899585\n",
      "68        0.597971\n",
      "69        0.953750\n",
      "70        0.968961\n",
      "71        0.905913\n",
      "72        0.958988\n",
      "73        0.576960\n",
      "74        0.680866\n",
      "75        0.563542\n",
      "76        0.781194\n",
      "77        0.754267\n",
      "78        0.747362\n",
      "79        0.738831\n",
      "80        0.882629\n",
      "81        0.868378\n",
      "82        0.669624\n",
      "83        0.885352\n",
      "84        0.900763\n",
      "85        0.913599\n",
      "86        0.887276\n",
      "87        0.655367\n",
      "88        0.897414\n",
      "89        0.866226\n",
      "90        0.681538\n",
      "91        0.643659\n",
      "92        0.915086\n",
      "93        0.795666\n",
      "94        0.994213\n",
      "95        0.839322\n",
      "96        0.984004\n",
      "97        0.968568\n",
      "98        0.610238\n",
      "99        0.885185\n",
      "100       0.808094\n",
      "101       0.991555\n",
      "102       0.999944\n",
      "103       0.944986\n",
      "104       0.846711\n",
      "105       0.577952\n",
      "106       0.438134\n",
      "107       0.759770\n",
      "108       0.648767\n",
      "109       0.365601\n",
      "110       0.785250\n",
      "111       0.747643\n",
      "112       0.854569\n",
      "113       0.992310\n",
      "114       0.915638\n",
      "115       0.931186\n",
      "116       0.704795\n",
      "117       0.969466\n",
      "118       0.747418\n",
      "119       0.999718\n",
      "120       0.837539\n",
      "121       0.867964\n",
      "122       1.000000\n",
      "123       0.984733\n",
      "124       1.000000\n",
      "125       0.939170\n",
      "126       0.357387\n",
      "127       0.926440\n",
      "128       0.707957\n",
      "129       0.539234 \n",
      "\n",
      "Blue Model Test Final\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       131\n",
      "           1       0.99      0.98      0.99       118\n",
      "           2       0.87      0.88      0.88       128\n",
      "           3       0.94      0.89      0.91       131\n",
      "           4       0.88      0.99      0.93       128\n",
      "           5       0.94      1.00      0.97       132\n",
      "           6       0.96      0.96      0.96       122\n",
      "           7       0.90      0.96      0.93       131\n",
      "           8       0.97      0.97      0.97       131\n",
      "           9       0.99      0.90      0.95       115\n",
      "          10       0.82      0.66      0.73       131\n",
      "          11       0.95      0.93      0.94       131\n",
      "          12       0.97      0.96      0.96       179\n",
      "          13       0.98      0.98      0.98       131\n",
      "          14       0.97      0.96      0.97       114\n",
      "          15       0.94      0.77      0.85       132\n",
      "          16       0.95      0.95      0.95       131\n",
      "          17       0.99      0.81      0.89       121\n",
      "          18       0.95      0.89      0.92       131\n",
      "          19       0.65      0.70      0.67       120\n",
      "          20       0.91      0.98      0.95       123\n",
      "          21       0.94      0.85      0.90       131\n",
      "          22       0.59      0.98      0.74       131\n",
      "          23       0.85      0.92      0.88       131\n",
      "          24       0.95      0.89      0.92       131\n",
      "          25       0.93      0.93      0.93       187\n",
      "          26       1.00      0.99      1.00       131\n",
      "          27       0.95      0.99      0.97       197\n",
      "          28       0.81      0.82      0.82       197\n",
      "          29       1.00      1.00      1.00       131\n",
      "          30       1.00      1.00      1.00       131\n",
      "          31       1.00      0.96      0.98       132\n",
      "          32       0.94      0.88      0.91       121\n",
      "          33       0.94      0.99      0.97       131\n",
      "          34       0.98      0.64      0.77       131\n",
      "          35       0.92      0.81      0.86       120\n",
      "          36       0.88      0.89      0.89       123\n",
      "          37       0.96      0.87      0.91       104\n",
      "          38       1.00      0.98      0.99       125\n",
      "          39       0.96      0.84      0.89       131\n",
      "          40       0.97      0.93      0.95       125\n",
      "          41       0.81      0.94      0.87       187\n",
      "          42       0.96      0.58      0.72        79\n",
      "          43       0.74      0.93      0.83       131\n",
      "          44       0.96      0.98      0.97       262\n",
      "          45       0.92      0.94      0.93       131\n",
      "          46       0.71      0.95      0.81       131\n",
      "          47       0.79      1.00      0.89       131\n",
      "          48       0.98      0.94      0.96       131\n",
      "          49       0.98      0.99      0.99       126\n",
      "          50       0.96      0.98      0.97       131\n",
      "          51       0.98      0.96      0.97       131\n",
      "          52       0.98      0.93      0.96       131\n",
      "          53       0.95      0.90      0.93       124\n",
      "          54       0.87      0.92      0.90       131\n",
      "          55       0.76      0.93      0.84       131\n",
      "          56       0.86      0.89      0.87       124\n",
      "          57       0.93      0.60      0.72       126\n",
      "          58       0.82      0.90      0.86       131\n",
      "          59       0.82      0.93      0.87       131\n",
      "          60       1.00      0.98      0.99       131\n",
      "          61       0.93      0.53      0.68       131\n",
      "          62       0.83      0.77      0.80       131\n",
      "          63       0.91      0.93      0.92       131\n",
      "          64       0.98      0.91      0.94       131\n",
      "          65       0.98      0.98      0.98       114\n",
      "          66       0.89      0.72      0.80        80\n",
      "          67       0.96      0.93      0.95       131\n",
      "          68       0.88      0.92      0.90       197\n",
      "          69       0.99      0.93      0.96       131\n",
      "          70       0.88      0.96      0.92       131\n",
      "          71       0.98      0.92      0.95       128\n",
      "          72       0.99      0.99      0.99       175\n",
      "          73       0.87      0.78      0.83       142\n",
      "          74       0.95      0.47      0.63       120\n",
      "          75       0.73      0.93      0.82       117\n",
      "          76       1.00      1.00      1.00       128\n",
      "          77       0.97      0.98      0.97       131\n",
      "          78       0.76      0.89      0.82       131\n",
      "          79       0.94      0.86      0.90       131\n",
      "          80       0.97      0.92      0.95       197\n",
      "          81       0.97      0.90      0.93       131\n",
      "          82       0.92      0.84      0.88       131\n",
      "          83       0.91      0.86      0.89       186\n",
      "          84       0.96      0.97      0.97       131\n",
      "          85       0.97      0.98      0.97       188\n",
      "          86       0.97      0.91      0.94        80\n",
      "          87       0.94      0.78      0.85       131\n",
      "          88       0.87      0.92      0.89       178\n",
      "          89       0.95      0.79      0.87       190\n",
      "          90       0.93      0.84      0.88       132\n",
      "          91       0.86      0.90      0.88       132\n",
      "          92       0.95      0.92      0.93       118\n",
      "          93       0.96      0.96      0.96       187\n",
      "          94       0.99      0.95      0.97       178\n",
      "          95       0.98      0.98      0.98       178\n",
      "          96       0.93      0.96      0.95       131\n",
      "          97       0.94      0.90      0.92       131\n",
      "          98       0.85      0.85      0.85       131\n",
      "          99       0.98      0.78      0.87       132\n",
      "         100       0.77      0.95      0.85       131\n",
      "         101       0.95      0.93      0.94       120\n",
      "         102       0.97      1.00      0.99       113\n",
      "         103       0.99      0.91      0.95       241\n",
      "         104       0.64      0.96      0.77       131\n",
      "         105       0.86      0.98      0.92       121\n",
      "         106       0.85      0.60      0.70       120\n",
      "         107       0.89      0.89      0.89       121\n",
      "         108       0.97      0.53      0.69       120\n",
      "         109       0.75      0.61      0.67       120\n",
      "         110       0.95      0.89      0.92       131\n",
      "         111       0.97      0.87      0.92       131\n",
      "         112       1.00      0.92      0.96       131\n",
      "         113       1.00      1.00      1.00       131\n",
      "         114       0.98      0.95      0.97       131\n",
      "         115       0.99      0.92      0.96       131\n",
      "         116       0.91      0.77      0.84       197\n",
      "         117       1.00      0.98      0.99       131\n",
      "         118       0.98      1.00      0.99       131\n",
      "         119       0.99      1.00      0.99       197\n",
      "         120       0.96      0.87      0.91       179\n",
      "         121       0.99      0.98      0.99       197\n",
      "         122       1.00      1.00      1.00       128\n",
      "         123       0.97      1.00      0.98       131\n",
      "         124       1.00      0.99      1.00       183\n",
      "         125       0.97      0.98      0.97        99\n",
      "         126       0.99      1.00      1.00       123\n",
      "         127       0.95      0.98      0.97       126\n",
      "         128       0.92      1.00      0.96       197\n",
      "         129       0.89      0.87      0.88       126\n",
      "\n",
      "   micro avg       0.92      0.91      0.91     17956\n",
      "   macro avg       0.92      0.90      0.91     17956\n",
      "weighted avg       0.93      0.91      0.91     17956\n",
      " samples avg       0.88      0.91      0.89     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.961608\n",
      "1         0.982995\n",
      "2         0.881859\n",
      "3         0.885047\n",
      "4         0.991234\n",
      "5         0.999551\n",
      "6         0.958736\n",
      "7         0.961047\n",
      "8         0.969241\n",
      "9         0.904292\n",
      "10        0.663056\n",
      "11        0.930905\n",
      "12        0.954970\n",
      "13        0.984621\n",
      "14        0.964744\n",
      "15        0.764815\n",
      "16        0.953806\n",
      "17        0.809861\n",
      "18        0.885160\n",
      "19        0.697477\n",
      "20        0.983067\n",
      "21        0.854569\n",
      "22        0.972106\n",
      "23        0.914796\n",
      "24        0.892793\n",
      "25        0.929750\n",
      "26        0.992366\n",
      "27        0.989228\n",
      "28        0.820195\n",
      "29        1.000000\n",
      "30        1.000000\n",
      "31        0.962121\n",
      "32        0.883905\n",
      "33        0.991918\n",
      "34        0.641109\n",
      "35        0.807829\n",
      "36        0.893468\n",
      "37        0.865161\n",
      "38        0.984000\n",
      "39        0.839414\n",
      "40        0.927776\n",
      "41        0.933521\n",
      "42        0.582167\n",
      "43        0.928941\n",
      "44        0.980238\n",
      "45        0.938370\n",
      "46        0.943760\n",
      "47        0.998093\n",
      "48        0.938763\n",
      "49        0.991951\n",
      "50        0.984452\n",
      "51        0.961720\n",
      "52        0.931186\n",
      "53        0.902889\n",
      "54        0.922654\n",
      "55        0.929166\n",
      "56        0.886087\n",
      "57        0.594902\n",
      "58        0.899305\n",
      "59        0.929783\n",
      "60        0.984733\n",
      "61        0.534071\n",
      "62        0.769870\n",
      "63        0.930624\n",
      "64        0.908229\n",
      "65        0.982344\n",
      "66        0.724608\n",
      "67        0.931017\n",
      "68        0.922450\n",
      "69        0.931242\n",
      "70        0.960878\n",
      "71        0.921763\n",
      "72        0.988515\n",
      "73        0.780792\n",
      "74        0.474832\n",
      "75        0.929326\n",
      "76        1.000000\n",
      "77        0.976875\n",
      "78        0.891110\n",
      "79        0.862203\n",
      "80        0.923520\n",
      "81        0.900539\n",
      "82        0.839190\n",
      "83        0.859371\n",
      "84        0.969185\n",
      "85        0.978386\n",
      "86        0.912388\n",
      "87        0.778233\n",
      "88        0.914380\n",
      "89        0.794287\n",
      "90        0.840404\n",
      "91        0.900449\n",
      "92        0.914918\n",
      "93        0.956825\n",
      "94        0.949326\n",
      "95        0.982977\n",
      "96        0.961327\n",
      "97        0.900371\n",
      "98        0.846262\n",
      "99        0.780191\n",
      "100       0.944489\n",
      "101       0.924664\n",
      "102       0.999832\n",
      "103       0.912750\n",
      "104       0.957905\n",
      "105       0.982406\n",
      "106       0.599271\n",
      "107       0.891833\n",
      "108       0.533221\n",
      "109       0.606988\n",
      "110       0.885160\n",
      "111       0.870061\n",
      "112       0.916031\n",
      "113       1.000000\n",
      "114       0.954086\n",
      "115       0.923608\n",
      "116       0.770729\n",
      "117       0.977099\n",
      "118       0.999888\n",
      "119       0.999887\n",
      "120       0.871171\n",
      "121       0.984659\n",
      "122       1.000000\n",
      "123       0.999776\n",
      "124       0.994536\n",
      "125       0.979630\n",
      "126       0.999944\n",
      "127       0.983790\n",
      "128       0.999043\n",
      "129       0.872287 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_red_test_final = mlpc_red_final.predict(X_red_test_final)\n",
    "vector_green_test_final = mlpc_green_final.predict(X_green_test_final)\n",
    "vector_blue_test_final = mlpc_blue_final.predict(X_blue_test_final)\n",
    "\n",
    "print(\"Red Model Test Final\")\n",
    "print(classification_report(Y_test_final, vector_red_test_final, zero_division=0))\n",
    "print(get_youdens_index(vector_red_test_final, Y_test_final), \"\\n\")\n",
    "\n",
    "print(\"Green Model Test Final\")\n",
    "print(classification_report(Y_test_final, vector_green_test_final, zero_division=0))\n",
    "print(get_youdens_index(vector_green_test_final, Y_test_final), \"\\n\")\n",
    "\n",
    "print(\"Blue Model Test Final\")\n",
    "print(classification_report(Y_test_final, vector_blue_test_final, zero_division=0))\n",
    "print(get_youdens_index(vector_blue_test_final, Y_test_final), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52d927cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       131\n",
      "           1       0.99      0.99      0.99       118\n",
      "           2       0.93      0.88      0.90       128\n",
      "           3       0.97      0.96      0.97       131\n",
      "           4       0.98      0.95      0.97       128\n",
      "           5       0.96      0.99      0.98       132\n",
      "           6       1.00      0.98      0.99       122\n",
      "           7       0.99      0.95      0.97       131\n",
      "           8       0.98      0.95      0.97       131\n",
      "           9       0.99      0.91      0.95       115\n",
      "          10       1.00      0.94      0.97       131\n",
      "          11       0.99      0.97      0.98       131\n",
      "          12       0.99      0.96      0.97       179\n",
      "          13       1.00      0.98      0.99       131\n",
      "          14       0.99      0.94      0.96       114\n",
      "          15       0.98      0.90      0.94       132\n",
      "          16       0.98      0.97      0.98       131\n",
      "          17       0.99      0.82      0.90       121\n",
      "          18       1.00      0.89      0.94       131\n",
      "          19       0.84      0.72      0.78       120\n",
      "          20       0.99      0.98      0.99       123\n",
      "          21       0.96      0.85      0.90       131\n",
      "          22       0.91      0.98      0.95       131\n",
      "          23       0.96      0.95      0.95       131\n",
      "          24       0.98      0.88      0.93       131\n",
      "          25       0.97      0.94      0.96       187\n",
      "          26       1.00      0.99      1.00       131\n",
      "          27       0.99      0.98      0.99       197\n",
      "          28       0.97      0.94      0.96       197\n",
      "          29       1.00      0.99      1.00       131\n",
      "          30       0.99      1.00      1.00       131\n",
      "          31       1.00      0.95      0.98       132\n",
      "          32       0.98      0.92      0.95       121\n",
      "          33       0.98      0.99      0.98       131\n",
      "          34       0.94      0.82      0.88       131\n",
      "          35       1.00      0.83      0.91       120\n",
      "          36       0.96      0.87      0.91       123\n",
      "          37       0.99      0.83      0.90       104\n",
      "          38       1.00      0.98      0.99       125\n",
      "          39       0.92      0.91      0.91       131\n",
      "          40       1.00      0.92      0.96       125\n",
      "          41       0.98      0.90      0.94       187\n",
      "          42       0.92      0.84      0.87        79\n",
      "          43       0.98      0.92      0.95       131\n",
      "          44       1.00      0.99      1.00       262\n",
      "          45       0.97      0.95      0.96       131\n",
      "          46       0.95      0.92      0.93       131\n",
      "          47       0.98      0.99      0.98       131\n",
      "          48       1.00      0.98      0.99       131\n",
      "          49       1.00      1.00      1.00       126\n",
      "          50       0.98      0.98      0.98       131\n",
      "          51       0.98      0.96      0.97       131\n",
      "          52       0.98      0.95      0.97       131\n",
      "          53       0.95      0.94      0.95       124\n",
      "          54       0.99      0.96      0.98       131\n",
      "          55       0.93      0.92      0.93       131\n",
      "          56       0.95      0.91      0.93       124\n",
      "          57       0.94      0.94      0.94       126\n",
      "          58       0.93      0.88      0.91       131\n",
      "          59       0.96      0.95      0.95       131\n",
      "          60       1.00      0.98      0.99       131\n",
      "          61       0.97      0.63      0.76       131\n",
      "          62       0.95      0.79      0.87       131\n",
      "          63       0.97      0.94      0.95       131\n",
      "          64       0.98      0.98      0.98       131\n",
      "          65       0.99      0.97      0.98       114\n",
      "          66       0.96      0.80      0.87        80\n",
      "          67       0.97      0.97      0.97       131\n",
      "          68       0.94      0.92      0.93       197\n",
      "          69       0.99      0.95      0.97       131\n",
      "          70       0.99      0.95      0.97       131\n",
      "          71       0.99      0.95      0.97       128\n",
      "          72       1.00      0.97      0.99       175\n",
      "          73       0.93      0.79      0.85       142\n",
      "          74       0.80      0.69      0.74       120\n",
      "          75       0.91      0.90      0.91       117\n",
      "          76       1.00      1.00      1.00       128\n",
      "          77       0.98      0.97      0.98       131\n",
      "          78       0.92      0.92      0.92       131\n",
      "          79       0.99      0.89      0.94       131\n",
      "          80       0.99      0.99      0.99       197\n",
      "          81       0.99      0.95      0.97       131\n",
      "          82       0.94      0.86      0.90       131\n",
      "          83       0.96      0.91      0.93       186\n",
      "          84       1.00      0.94      0.97       131\n",
      "          85       0.98      0.97      0.98       188\n",
      "          86       0.99      0.88      0.93        80\n",
      "          87       0.98      0.78      0.87       131\n",
      "          88       0.96      0.94      0.95       178\n",
      "          89       0.95      0.93      0.94       190\n",
      "          90       0.97      0.86      0.91       132\n",
      "          91       0.90      0.87      0.88       132\n",
      "          92       1.00      0.93      0.96       118\n",
      "          93       0.99      0.95      0.97       187\n",
      "          94       0.99      0.99      0.99       178\n",
      "          95       0.99      0.98      0.99       178\n",
      "          96       0.98      0.95      0.97       131\n",
      "          97       1.00      0.96      0.98       131\n",
      "          98       0.93      0.82      0.87       131\n",
      "          99       0.98      0.93      0.96       132\n",
      "         100       0.95      0.89      0.92       131\n",
      "         101       0.98      0.99      0.99       120\n",
      "         102       1.00      1.00      1.00       113\n",
      "         103       0.98      1.00      0.99       241\n",
      "         104       0.93      0.96      0.95       131\n",
      "         105       0.98      0.98      0.98       121\n",
      "         106       0.95      0.59      0.73       120\n",
      "         107       0.93      0.86      0.89       121\n",
      "         108       0.89      0.67      0.76       120\n",
      "         109       0.84      0.63      0.72       120\n",
      "         110       1.00      0.91      0.95       131\n",
      "         111       0.95      0.95      0.95       131\n",
      "         112       0.99      0.92      0.96       131\n",
      "         113       1.00      1.00      1.00       131\n",
      "         114       0.99      0.95      0.97       131\n",
      "         115       1.00      0.94      0.97       131\n",
      "         116       0.96      0.88      0.92       197\n",
      "         117       1.00      0.98      0.99       131\n",
      "         118       1.00      0.98      0.99       131\n",
      "         119       1.00      0.99      1.00       197\n",
      "         120       0.98      0.91      0.94       179\n",
      "         121       0.99      0.97      0.98       197\n",
      "         122       1.00      1.00      1.00       128\n",
      "         123       1.00      1.00      1.00       131\n",
      "         124       1.00      1.00      1.00       183\n",
      "         125       1.00      0.98      0.99        99\n",
      "         126       1.00      0.99      1.00       123\n",
      "         127       0.97      0.98      0.98       126\n",
      "         128       0.94      0.98      0.96       197\n",
      "         129       0.98      0.90      0.94       126\n",
      "\n",
      "   micro avg       0.97      0.93      0.95     17956\n",
      "   macro avg       0.97      0.93      0.95     17956\n",
      "weighted avg       0.97      0.93      0.95     17956\n",
      " samples avg       0.93      0.93      0.93     17956\n",
      "\n",
      "     Youdens Index\n",
      "0         0.992310\n",
      "1         0.991469\n",
      "2         0.882308\n",
      "3         0.961608\n",
      "4         0.953013\n",
      "5         0.992144\n",
      "6         0.983607\n",
      "7         0.946509\n",
      "8         0.954086\n",
      "9         0.912987\n",
      "10        0.938931\n",
      "11        0.969410\n",
      "12        0.960781\n",
      "13        0.984733\n",
      "14        0.938540\n",
      "15        0.901403\n",
      "16        0.969353\n",
      "17        0.818126\n",
      "18        0.893130\n",
      "19        0.724047\n",
      "20        0.983684\n",
      "21        0.854681\n",
      "22        0.984004\n",
      "23        0.946284\n",
      "24        0.877750\n",
      "25        0.940895\n",
      "26        0.992366\n",
      "27        0.984715\n",
      "28        0.938805\n",
      "29        0.992366\n",
      "30        0.999944\n",
      "31        0.954545\n",
      "32        0.917243\n",
      "33        0.992198\n",
      "34        0.824035\n",
      "35        0.833333\n",
      "36        0.869638\n",
      "37        0.826867\n",
      "38        0.984000\n",
      "39        0.907780\n",
      "40        0.920000\n",
      "41        0.898171\n",
      "42        0.835107\n",
      "43        0.915918\n",
      "44        0.992366\n",
      "45        0.953974\n",
      "46        0.923271\n",
      "47        0.992198\n",
      "48        0.984733\n",
      "49        1.000000\n",
      "50        0.984621\n",
      "51        0.961720\n",
      "52        0.954086\n",
      "53        0.943212\n",
      "54        0.961776\n",
      "55        0.923159\n",
      "56        0.910954\n",
      "57        0.936115\n",
      "58        0.877414\n",
      "59        0.946284\n",
      "60        0.984733\n",
      "61        0.633419\n",
      "62        0.793613\n",
      "63        0.938707\n",
      "64        0.984565\n",
      "65        0.973628\n",
      "66        0.799832\n",
      "67        0.969241\n",
      "68        0.918106\n",
      "69        0.954142\n",
      "70        0.946509\n",
      "71        0.953069\n",
      "72        0.971429\n",
      "73        0.788283\n",
      "74        0.690489\n",
      "75        0.896875\n",
      "76        1.000000\n",
      "77        0.969353\n",
      "78        0.915470\n",
      "79        0.893074\n",
      "80        0.989791\n",
      "81        0.946509\n",
      "82        0.862203\n",
      "83        0.908208\n",
      "84        0.938931\n",
      "85        0.973235\n",
      "86        0.874944\n",
      "87        0.778514\n",
      "88        0.943426\n",
      "89        0.925753\n",
      "90        0.855836\n",
      "91        0.870483\n",
      "92        0.932203\n",
      "93        0.946412\n",
      "94        0.988708\n",
      "95        0.983034\n",
      "96        0.954086\n",
      "97        0.961832\n",
      "98        0.823979\n",
      "99        0.931706\n",
      "100       0.892793\n",
      "101       0.991555\n",
      "102       1.000000\n",
      "103       0.995625\n",
      "104       0.961327\n",
      "105       0.975038\n",
      "106       0.591442\n",
      "107       0.859056\n",
      "108       0.666106\n",
      "109       0.632492\n",
      "110       0.908397\n",
      "111       0.953806\n",
      "112       0.923608\n",
      "113       1.000000\n",
      "114       0.954142\n",
      "115       0.938931\n",
      "116       0.877722\n",
      "117       0.977099\n",
      "118       0.984733\n",
      "119       0.994924\n",
      "120       0.910390\n",
      "121       0.969487\n",
      "122       1.000000\n",
      "123       1.000000\n",
      "124       1.000000\n",
      "125       0.979798\n",
      "126       0.991870\n",
      "127       0.983903\n",
      "128       0.979020\n",
      "129       0.904650 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_combined_test_final = np.column_stack((vector_red_test_final, vector_green_test_final, vector_blue_test_final)) # shape (582, 18) - 18 features\n",
    "\n",
    "# Test Final\n",
    "print(\"Combined Model Test\")\n",
    "combi_test_final = mlpc_combi_final.predict(X_combined_test_final)\n",
    "print(classification_report(Y_test_final, combi_test_final, zero_division=0))\n",
    "print(get_youdens_index(combi_test_final, Y_test_final), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a453b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
